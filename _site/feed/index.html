<?xml version="1.0" encoding="utf-8"?>
  <rss version="2.0"
        xmlns:content="http://purl.org/rss/1.0/modules/content/"
        xmlns:atom="http://www.w3.org/2005/Atom"
  >
  <channel>
    <title>Jaosonzhuo's blog</title>
    <link href="http://jason-zhuo.github.io//feed/" rel="self" />
    <link href="http://jason-zhuo.github.io/" />
    <lastBuildDate>2017-05-04T16:40:28-05:00</lastBuildDate>
    <webMaster>jason_zhuo@iclould.com</webMaster>
    
    <item>
      <title>Mac TeamViewer SSH 远程登录恢复</title>
      <link href="http://jason-zhuo.github.io//macteamviewer/"/>
      <pubDate>2017-04-03T00:00:00-05:00</pubDate>
      <author></author>
      <guid>http://jason-zhuo.github.io//MacTeamViewer</guid>
      <content:encoded><![CDATA[<p><img src="/assets/images/TeamMac.png" alt="image" /></p>

<p>Mac 上重新启动TeamViewer 以及Trouble Shooting。</p>

<!-- more -->


<h2>思路</h2>

<p>有些时候出于未知原因，我们无法使用TeamViewer登录远程的Mac计算机。但是我们却可以通过SSH连接到Mac上，这样也可以通过SSH来重新启动和登录TeamViewer。</p>

<p>总结起来就以下一些步骤：</p>

<ol>
<li>远程SSH到Mac电脑</li>
<li>运用Mac自带的Apple script来重新启动 TeamViewer.</li>
<li>将TeamViewer 显示到第一个窗口（为了防止密码被其他窗口遮挡）</li>
<li>截屏</li>
<li>用SCP 回传截屏图像</li>
<li>用新的临时密码进行TeamViewer登录</li>
</ol>


<h2>具体步骤</h2>

<p>具体命令行代码：</p>

<pre><code class="python">停止TeamViewer 
osascript -e 'tell application "TeamViewer" to quit'
启动TeamViewer 
osascript -e 'tell application "TeamViewer" to run'
将TeamViewer移动到第一个窗口
osascript -e 'tell application "TeamViewer" to activate'
将TeamViewer重新打开到第一个窗口
osascript -e 'tell application "TeamViewer" to reopen'
截屏
screencapture xxx.jpg
回传用scp
scp xx@host:(path to xxx.jpg) (path to save jpg)
</code></pre>

<h2><strong>Trouble Shooting:</strong></h2>

<p>有的时候回出现如下的一些情况，比如新弹出的对话框挡住了密码。笔者尝试使用了各种办法来模拟用户单击对话框的“OK”都失败了。</p>

<p>由于权限原因：System Events got an error: osascript is not allowed assistive access.</p>

<p>这个时候可能真需要真人去一趟了。</p>

<p><img src="/assets/images/050037934C0DDBBA196577D400A84DB7.jpg" alt="" /></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Flow sketching and traffic correlation attack</title>
      <link href="http://jason-zhuo.github.io//flow-sketching-and-traceback/"/>
      <pubDate>2017-03-10T00:00:00-06:00</pubDate>
      <author></author>
      <guid>http://jason-zhuo.github.io//Flow sketching and traceback</guid>
      <content:encoded><![CDATA[<p><img src="/assets/images/2017-03-10IPtracebackandFlowsketch.png" alt="image" />
Recent paper reading on traffic correlation attack and flow skeching, last update (2017.3.13)</p>

<!-- more -->


<h3>1. 本文概要</h3>

<p>Paper Reading notes on:</p>

<ol>
<li><strong>Online Sketching of Network Flows for Real-Time Stepping-Stone Detection[1]</strong></li>
<li><strong>Traffic confirmation attacks despite noise[2]</strong></li>
<li><strong>Tracking encrypted VoIP calls via robust hashing of network flow[3]</strong></li>
</ol>


<h3>2. Background</h3>

<h4>2.1 Stepping stone</h4>

<p>直接翻译过来叫做垫脚石攻击，感觉有点奇怪。</p>

<blockquote><p>垫脚石（Stepping stone）: The intermediate hosts used for traffic laundering are known as stepping stones. 垫脚石就是攻击者利用一系列中间主机作为跳板，为了掩盖攻击者的真实身份和位置的一种攻击方式。</p></blockquote>

<p>垫脚石攻击的检测：在网络设备中记录流的一些信息。然后根据关联算法，能够找到真正的攻击源。传统的垫脚石攻击检测算法记录流的一些基本特征，例如流的包数量，包间隔时间等等。 但是传统的垫脚石攻击检测对丢包，噪声和网络环境变化并不能很好适用。（However, they provide very limited or no resistance to some of the aforementioned timing perturbations, especially packet drops/retransmissions and chaff.）而且传统的垫脚石攻击检测算法相对于本文的算法保存了更多的信息，不利于大量流量的数据处理，因此流量关联算法的时间复杂度比较高 O（m*n）, m是出去方向的流个数，n是进入方向的流量个数。</p>

<h4>2.2切比雪夫不等式（Chebyshev inequality）</h4>

<p>若随机变量X的数学期望、方差分别为E(X)及D(X)，则对任何\(\epsilon >0\)，成立</p>

<p>$$P(|X-E(X)| \ge \epsilon) \le \frac{D(X)}{\epsilon^{2}}$$</p>

<p>几何解释及意义详见： https://www.zhihu.com/question/27821324</p>

<h4>2.3 相关工作</h4>

<p>本文所提出的技术与鲁棒散列方案（Robust Hashing）类似。 它们都通过短序列（鲁棒散列）表示输入信号，这个序列的性质可以让其抵抗输入上的小扰动。 在多媒体信号处理的运用中，即便原始信号收到干扰或者变化（例如压缩，次滤波等），鲁棒散列函数仍然能够识别和认证多媒体内容。</p>

<h3>3.流量Sketch计算和表示方法</h3>

<p>实时的流量关联：在网络边界上，记录出入的active流的信息。active流在文章中设置的是60s的超时时间。如果在时刻\(t\)，出去方向的第\(i\)号流\(E_i (t)\)，和进入方向的第\(j\)号流\(I_j (t)\), 满足\(Diff_t(E_i (t),I_j (t)) &lt; \epsilon \) 那么这两个流就被关联起来了。 Diff方法在文章中采用的汉明距离，然后两个流的sketches都是用binary的形式进行保存的。 采用新的表示方法，节约了保存空间，提高了计算效率。</p>

<p>一个流的Sketch是如何进行计算的呢？这里的high-level idea是将流量的packet-timing 特征进行线性变换，如果两个输入向量相似那么新产生的这两个向量也会相似。 具体做法是将一个流的传输分为k个time slots，每个time slot (t) 统计传输的包的数量 \(V_F(t)\), 然后每个time slot (t)对应一个随机映射函数（也可以是随机数）。 再计算k个累计的包传输数量和 \(C_F(i)\) (i=1,2,...,k), 最后根据\(C_F(i)\) 的符号进行编码（大于0的编码为1，小于等于0的编码为0），最终形成一个长度为k的0，1序列。</p>

<p>$$C_F(i)= \sum_{t=1}^{t=k} V_F(t)B_i(t)$$</p>

<p>一个例子：</p>

<p><img src="/assets/images/2017-03-10FlowSketch.png" alt="image" /></p>

<p>最后根据符号函数得到该流的sketch 为 101, 如何后续还有数据包进出的话，那么就在101的基础之上向后同理进行计算，如上图省略号所示。</p>

<h4>3.1流量Sketch的基本性质</h4>

<p>假设两个Flow，F1和F2相似，那么F1和F2的最后sketch也应相似。但是现实中基本不存在两个流完全一样，那么这两流的sketches的比特错误的概率大小\(P_{e}(F1,F2)\) 应该较低才满足相似的规定。比特错误的引入归根结底是因为\(V_F(t)\)，那么我们定义F1,F2包数量之间的不同为：</p>

<p>$$\epsilon =V_F2(t)-V_F1(t)$$</p>

<p>带入上式（\(C_F(i)\)的那个）</p>

<p>那么有</p>

<p>$$C_F2(i) = C_F1(i)+ \sum B_i(t) \epsilon (t)$$</p>

<p>要使得两个bit异号，那么上述\(C_F2(i)\)， \(C_F1(i)\)得异号。 那么异号的条件就是 \(|\sum B<em>{i}(t) \varepsilon (t)| > C</em>{F1}(i)\) 以及 \(\sum B_i(t) \epsilon (t)\) 和 \(C_F1(i)\)</p>

<p>最后根据切比雪夫不等式（省略一些步骤）得到一个重要不等式：</p>

<p>$$P_e(F1,F2) \le \frac{\sum_t \epsilon (t) * \epsilon (t) }{|C_F(i)|^2}$$</p>

<p>上式表明，F1, F2 的包数量之间的不同越大（分子），那么比特错误的概率就越高。 反之，如果 \(|C_F1(i)|\)的个数增多（即线性变换的随机基向量越多），那么表明这两个流的sketches 比特错误的概率越低，从而一定程度上降低了sketches对噪声的敏感程度。</p>

<p>注明：web版latex公式上下标有点问题，文中公式有可能存在错误.</p>

<h3>4.流量Sketch搜索</h3>

<p>本质上文中采用的是空间换时间的方法，将原来的O(mn)算法，降低到 \(O(n +\sqrt{mn})\)， 具体思路还是比较好理解的。这里不再赘述。</p>

<hr />

<h3>5. 流量关联攻击</h3>

<p>流量关联攻击可以帮助我们找到两个相似的流。论文[2][3]创新地将[1]的方法运用到追踪溯源中，其具体做法如下图所示。</p>

<p><img src="/assets/images/2017-03-10FlowcorrelationAttack.png" alt="image" /></p>

<p>攻击者是passive listener，在入口和出口处对流量进行Hash(其实和[1]中的流量sketches很类似，有些小不同后面会介绍)，然后找到哪个user访问了target website，从而达到溯源目的。</p>

<p>如何计算流的binary hash呢？</p>

<p><strong>[3]的算法如下：</strong></p>

<p><img src="/assets/images/2017-03-10binaryHash1.png" alt="image" /></p>

<p>算法[3]和[2]的不同之处就在于，[3]的B增量是以字节大小表示的，[2]是以包的个数表示的，由于报文填充机制，所以[2]没有利用包的大小信息。</p>

<p><strong>[2]的算法如下：</strong></p>

<p><img src="/assets/images/2017-03-10binaryhash.png" alt="image" /></p>

<ol>
<li>Line 1: 初始化binary hash 向量</li>
<li>Line 2: 为每个time window 提取该窗口的包数量</li>
<li>Line 3: 对于每个窗口进行计算</li>
<li>Line 4~6: 对于第一个窗口，计算稍微有些不同</li>
<li>Line 7~10: 对于后续窗口如何进行Hash值计算，Line 8计算和前一个窗口的包数量差，Line 9, 用增量update H。那个加号其实就是向量加法，后面的中括号其实就是随机向量，这里的m应该和H的长度一样才行，也就是说随机数的个数要和binary hash长度一样。</li>
<li>Line 11: 符号函数，如果H对应的bit位置的值大于0则编码为1， 如果小于等于0则编码为0。感觉应该可以放到for循环外。</li>
</ol>


<p>问题：如何得到窗口个数N？ Answer: 对于每个flow，N都是一样的。 N is decided on prior to computation，如果包数量比窗口数量还少，那么丢弃该流。</p>

<h4>Refs</h4>

<p>[1] Coskun B, Memon N D. Online Sketching of Network Flows for Real-Time Stepping-Stone Detection[C]//ACSAC. 2009: 473-483.</p>

<p>[2]Hayes J. traffic confirmation attacks despite noise[J]. arXiv preprint arXiv:1601.04893, 2016.</p>

<p>[3]Coskun B, Memon N. Tracking encrypted VoIP calls via robust hashing of network flows[C]//Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on. IEEE, 2010: 1818-1821.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Malware traffic analysis</title>
      <link href="http://jason-zhuo.github.io//malwaretrafficanalysis/"/>
      <pubDate>2017-03-01T00:00:00-06:00</pubDate>
      <author></author>
      <guid>http://jason-zhuo.github.io//MalwareTrafficAnalysis</guid>
      <content:encoded><![CDATA[<p><img src="http://ef67fc04ce9b132c2b32-8aedd782b7d22cfe0d1146da69a52436.r14.cf1.rackcdn.com/in-britain-malware-most-foul-showcase_image-5-a-8255.jpg" alt="image" />
Nazca: Detecting Malware Distribution in Large-Scale Networks, last update (2017.3.3)</p>

<!-- more -->


<h3>1. 本文概要</h3>

<p>Paper Reading notes on:</p>

<p><strong>Nazca: Detecting Malware Distribution in Large-Scale Networks</strong></p>

<h3>2. 传统方法</h3>

<p>drive-by download attack 的过程可以被分为3个步骤：</p>

<ol>
<li>利用阶段（exploitation phase）: 通过利用漏洞让客户机执行shellcode代码</li>
<li>安装阶段（installation phase）: shellcode被执行，开始从远程下载恶意软件</li>
<li>控制阶段 （control phase）: 恶意软件产生流量与控制端进行通信</li>
</ol>


<p>传统论文方法大多数工作主要是针对阶段1，3进行检测，如检测跨站脚本，挂马网页等，然后就是从主机发出的流量进行检测，如IDS，杀毒软件等。</p>

<h3>3. 论文特色</h3>

<ol>
<li>传统恶意软件检测算法主要在检测恶意软件已经被下载到用户电脑上之后产生的流量, 然而很少有研究去检测恶意软件是从哪儿被下载的。(针对阶段2)</li>
<li>提出了Nazca检测系统，detects infections in large scale networks。</li>
<li>利用了图的关联特性，可以检测出未知的malware，0-day</li>
</ol>


<h3>4. Nazca工作原理</h3>

<p>通常在阶段2，shellcode会发出HTTP请求，并从远程服务器下载恶意软件。由于是HTTP请求，和其他正常的请求很类似，所以很难被发现。论文的intuition 就是：好吧，少量的恶意软件我可能发现不了，但是很多主机都在请求某个恶意软件，那么就很可疑了。（Instead, when considering many malware downloads together – performed by different hosts, but related to a single campaign – a malware distribution infrastructure becomes visible. ）</p>

<p>Nazca主要负责在一个网络中检测HTTP请求，更具体的是下载可执行软件的HTTP请求。和普通HTTP下载不同的是，恶意软件下载有很多防御技术，1）domain fluxing, 2) malware repackaging 3) malware droppers for multiple installations. 作者表示，其方法可以对上述防御技术兼容。</p>

<p>Nazca检测主要分为两个部分，1）candidate selection 用 4.1，4.2节提出的方法检测一系列URIs，这些URIs表现出一些恶意行为。 2) Detection step, 运用1）产生的URIs，构建恶意邻居图，如果一个节点在恶意邻居图里面，那么这个图的其他节点很有可能也是恶意的。 可以看出1）这个步骤是为了过滤掉正常的，剩下恶意的方便后续分析。</p>

<h4>4.1 Nazca检测内容和特征提取</h4>

<p>Nazca检查MIME类型<a href="Content-type">^1</a>只要不在白名单中，那么就会提取：</p>

<ol>
<li>目的IP和源IP</li>
<li>目的端口和源端口</li>
<li>URI</li>
<li>HTTP头部的User-Agent</li>
<li>解压后文件的前 k 字节的哈希值</li>
</ol>


<p>论文focuses on HTTP 流量是因为：we observed that it is the protocol of choice for the  vast majority of malware in current circulations.</p>

<h4>4.2 恶意域名检测</h4>

<p>恶意软件为了躲避杀毒软件的查杀，使用不同的技术(malware hosting tries to evade blacklisting by using multiple domains, servers, payloads, file names, and URL paths)，作者在论文中针对恶意软件的：1）server-side polymorphism 2）multiple domains 进行了分析和处理。</p>

<p>所谓 server-side polymorphism就是恶意软件使用不同的加密key对软件进行加密或者攻击者准备多种变种恶意软件，每次下载不同的软件，从而达到绕过杀毒软件的signature库的目的。 作者使用的检测方法是：看一个下载记录与URI的关联情况，以及从URI处下载不同软件的数量。</p>

<p>检测恶意的CDN（区分正常的和恶意的CDN是非常重要的）。</p>

<p>检测步骤：</p>

<ol>
<li><strong>检测CDNs</strong></li>
</ol>


<p>如果相同1个文件的hash值对应多个不同的URIs，那么这些URIs的域名就属于一个簇。如果不同的2个簇里面都至少存在一个相同的域名，那么这两个簇就被连接起来。最后形成的这些簇就是CDNs.</p>

<ol>
<li><strong>区分恶意和正常CDNs</strong></li>
</ol>


<p>采用方法：机器学习</p>

<p>提取特征：1） <strong>域名关联度</strong>: 有些恶意软件作者用同一个IP hosts 不同的 domains，那么这个特征就是： hosts数目除以domains数。 2） <strong>不同的TLD域名个数</strong>：合法的CDN通常使用一个TLD,恶意的CDN通常使用多个不同的TLD(开源软件镜像除外)  3）<strong>匹配的URI和文件个数</strong>：合法的CDNs通常为了高效维护，采用的目录结构和文件名都一样。相反恶意软件为了防止blacklisting，通常采用不同的文件名和URI。 4）<strong>同一域名下的不同URI数量</strong>：恶意软件的网站目录结构复杂性低于其他正常域名（有例外，但很少）。 5）<strong>文件类型</strong>：正常CDNs通常有不同的文件类型，然而恶意的CDN通常只有可执行文件。</p>

<h3>4.3 构建恶意邻居图</h3>

<blockquote><p><strong>恶意邻居图</strong>：指关于某个可疑候选者一系列的恶意行为图（无向图）。</p></blockquote>

<p>节点内容包括：IP地址，域名，FQDN(Fully Qualified Domain Name)s, URLs, URL paths, file names，下载文件的hash值。</p>

<p>初始节点可以是一个恶意的域名，或者恶意的FQDN。当一个新的节点来的时候，要把其加入图中，依据的是节点之间的关系：要么是URL相同，要么是域名相同等等。</p>

<p>论文作者根据这个生成的图，为每个恶意候选者计算一个分数。如果图中的恶意节点（经过阈值判断过）较多的话，那么这个节点很肯能就是恶意的。同时从图的连接性来看，可以关联出不能被其他方法检测出来的未知恶意软件。</p>

<h3>4.4 适用范围</h3>

<p>论文方法适用于HTTP based malwares。 不能够检测HTTPS，加密协议， 不能够检测依赖于其他正常网站（Google Drive, Dropbox等）来发布的恶意软件。</p>

<h4>Refs</h4>

<p>[1] Invernizzi L, Miskovic S, Torres R, et al. Nazca: Detecting Malware Distribution in Large-Scale Networks[C]//NDSS. 2014, 14: 23-26.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Deep Learning</title>
      <link href="http://jason-zhuo.github.io//deeplearning/"/>
      <pubDate>2017-03-01T00:00:00-06:00</pubDate>
      <author></author>
      <guid>http://jason-zhuo.github.io//Deeplearning</guid>
      <content:encoded><![CDATA[<p><img src="https://www.gitbook.com/cover/book/deep-learning-cn/deep-learning-cn.jpg?build=1461768620204" alt="image" />
Deep learning stuff, last update (2017.3.1)</p>

<!-- more -->


<h3>本文概要</h3>

<p>本文主要介绍Deep learning 和整理我所看到的一些资料，持续更新 (Keep updating, once I have time)。</p>

<h3>深度学习的优势与不足</h3>

<p>优点：</p>

<ol>
<li>在研究中可以发现，如果在原有的特征中加入这些自动学习得到的特征可以大大提高精确度，甚至在分类问题中比目前最好的分类算法效果还要好[3]</li>
<li>非监督学习</li>
</ol>


<p>缺点：</p>

<ol>
<li>深度学习的一个主要优势在于可以利用海量训练数据（即大数据），但是依赖于反向传播算法，仍然对计算量有很高的要求。</li>
<li>比起统计方法来说 那当然就是模型难以诠释，找出来的 Feature 对人而言并不直观。</li>
</ol>


<h3>Tensorflow</h3>

<p>使用Tensorflow需要知道：</p>

<blockquote><p>The central unit of data in TensorFlow is the <strong>tensor</strong>. Tensorflow基础数据单元，其rank就是训练数据的维度 (训练数据是几维的)</p>

<p>A <strong>computational graph</strong> is a series of TensorFlow operations arranged into a graph of nodes. 计算图？</p>

<p>To actually evaluate the nodes, we must run the computational graph within a session. <strong>A session</strong> encapsulates the control and state of the TensorFlow runtime. 计算评估模型需要一次会话</p>

<p><strong>A placeholder</strong> is a promise to provide a value later. 就是说数据现在没有，以后有了再提供
，先登记一下</p>

<p><strong>A Variable</strong> tf.Variable变量， tf.constant常量，变量是没有初始化的，常量是一调用就被初始化了的，然后就不变了。变量如果需要初始化，需要调用</p></blockquote>

<pre><code class="python">init = tf.global_variables_initializer()
sess.run(init)
</code></pre>

<h4>Refs</h4>

<p>[1] LeCun Y, Bengio Y, Hinton G. Deep learning[J]. Nature, 2015, 521(7553): 436-444.</p>

<p>[2] 将Keras作为tensorflow的精简接口 https://keras-cn.readthedocs.io/en/latest/blog/keras_and_tensorflow/</p>

<p>[3] http://blog.csdn.net/zouxy09/article/details/8775524</p>

<p>[Cuda install guide ] https://www.tensorflow.org/versions/master/get_started/os_setup#optional-install-cuda-gpus-on-linux</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Docker and Go lang</title>
      <link href="http://jason-zhuo.github.io//golangrelated/"/>
      <pubDate>2017-02-23T00:00:00-06:00</pubDate>
      <author></author>
      <guid>http://jason-zhuo.github.io//GolangRelated</guid>
      <content:encoded><![CDATA[<p><img src="https://cdn-images-1.medium.com/max/1800/1*3BB0kiPsh2ftMT9dKg9_GA.jpeg" alt="image" />
Go language using and some thoughts, last update (2017.2.23)</p>

<!-- more -->


<h3>1. 本文概要</h3>

<p>本文主要介绍Go语言以及Docker的使用，以及整理我所看到的一些资料。</p>

<h3>2. 开篇</h3>

<p>我们经常听到一句话</p>

<blockquote><p>其实这个年代你用什么编程语言都可以，只要能够熟练运用。</p></blockquote>

<p>精通一门语言对于我们学计算机的人来说确实够用了，我们掌握的其实不是语言，而是语言背后的逻辑思想（算法）。但事实上笔者认为在编程语言上还是需要有一定的选择，物尽其用，觉得用什么语言方便实现就用什么语言。本来笔者也是不想学Go编程语言的，但是看到其优越的特性，感觉不学的话会跟不上时代了。</p>

<p>论文The effect of DNS on Tor’s Anonymity 中使用了Go语言进行代码编写。作者搜集Alexa前100万的DNS流量，无论是从数量级还是工程难度来说都是具有一定挑战的。这篇blog就是学习一下大神是如何利用Go的优势的。</p>

<h3>3. Go语言</h3>

<p>网上说了很多关于Go语言的优劣势分析，稍微谷歌一下就行了。这里我只总结一下我觉得Go语言的优势吧(部分参考[1])。</p>

<ol>
<li><strong>部署简单</strong>。Go 编译生成的是一个静态可执行文件，除了 glibc 外没有其他外部依赖。这让部署变得异常方便：目标机器上只需要一个基础的系统和必要的管理、监控工具，完全不需要操心应用所需的各种包、库的依赖关系，大大减轻了维护的负担。 <strong>笔者点评：人生苦短，何必浪费时间搭建环境呢，配置环境变量呢！这个特性无敌</strong></li>
<li><strong>服务器，并发编程实现容易，上手简单。</strong></li>
<li><strong>跨平台支持</strong>。在mac上编译的go运用程序，可以跑在各种类型的操作系统上，而且目标操作系统都不需要Go编译环境。 <strong>笔者点评：我也非常喜欢这个特性，java还得装jvm才能跨平台</strong></li>
</ol>


<h3>4. DNS 数据搜集</h3>

<p>在论文The effect of DNS on Tor’s Anonymity 中， 作者对Alexa top 100 million网站的DNS数据抓取采用了Go+docker的模式来进行。 作者实现了一个server来分任务给位于dockers中的workers. server服务端和docker客户端都采用的Go语言进行编写。 sever负责将alexa 网站地址传给worker，然后通过RPC调用将worker搜集到的pcap返回并保存。</p>

<h4>4.1 Docker network traffic dumping</h4>

<p>在默认情况下，从docker里面搜集网络数据会是怎么样的呢？我们来试试:</p>

<blockquote><p>docker exec --privileged -it myubuntu /bin/bash</p>

<p>ifconfig eth0 promisc</p></blockquote>

<p>从docker 里面 ping 8.8.8.8 并同时开启tcpdump 发现如下结果：</p>

<p><img src="/assets/images/2017227pcapdocker.png" alt="image" /></p>

<p>结果发现只有该容器自身的网络数据包。论文作者正是利用了这样的一种网络隔离性，来达到利用多个Docker同时搜集DNS流量的目的。这样做的的好处是可以并发进行数据搜集，加快流量搜集速度。</p>

<h4>图示</h4>

<p>如果我们可以这样来搜集网络数据包，岂不是很效率，事半功倍。</p>

<p><img src="/assets/images/2017227dockerss.png" alt="image" /></p>

<h3>5. 其他</h3>

<p>Docker 上面无需再要我们手工搭建环境了，模拟web请求可以用xvfb-run命令来达到虚拟屏幕输出，这样就可以从命令行访问网站了。xvfb-run好像在ubuntu 16.04上运行不了firefox，论文中作者使用的是Tor browser 5.5.4, 配置Tor browser 使用其他代理</p>

<h3>6. Docker 常见命令</h3>

<p>清除历史的containers</p>

<pre><code>docker stop $(docker ps -a -q) 

docker rm $(docker ps -a -q)
</code></pre>

<p>Reset所有</p>

<pre><code>pkill docker
iptables -t nat -F
ifconfig docker0 down
brctl delbr docker0
docker -d
</code></pre>

<p>开启Docker sslocal</p>

<pre><code>容器里面抓包需要权限 
docker run --privileged --name sslocal -d jasonzhuo/sslocal2 ./start.sh &lt;IP:Port&gt;
docker run --privileged --name sslocal -it jasonzhuo/sslocal2 /bin/bash
</code></pre>

<p>其他常用的</p>

<pre><code>重命名 docker tag server:latest myname/server:latest 
提交修改 docker commit [CONTAINER_ID] test/name
删除镜像 docker rmi xxx 
删除容器 docker rm xx
拷贝到容器 docker cp foo.txt mycontainer:/foo.txt
拷贝到主机 docker cp mycontainer:/foo.txt foo.txt
开一个新的terminal(假设现在在运行的container ID 为12345)
docker exec -it 12345 /bin/bash
</code></pre>

<h4>Refs</h4>

<p>[1] https://www.zhihu.com/question/21409296</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>“On Tor DNS“</title>
      <link href="http://jason-zhuo.github.io//tordns/"/>
      <pubDate>2017-02-19T00:00:00-06:00</pubDate>
      <author></author>
      <guid>http://jason-zhuo.github.io//TorDNS</guid>
      <content:encoded><![CDATA[<p><img src="https://nymity.ch/tor-dns/img/overview.png" alt="image" />
The effect of DNS on Tor’s Anonymity 论文阅读笔记, last update (2017.3.15)</p>

<!-- more -->


<h4>我又回来了</h4>

<p>之前在忙各种出国手续，写论文，做实验之类的事情，博客从去年8月到现在都没更新过，想想也是太烂龙了。最近翻到一本《精进：如何成为一个很厉害的人》，发现写博客还有如下功效，遂立即开始写作（以后也要坚持才行）：</p>

<blockquote><p><strong>写博客（写作）是一种典型的知识构建活动，或者更准确的说，是一种对知识的重构活动。</strong></p>

<p>在阅读时，我们对信息的输入和纳入（读论文），常常满足于从一个“浅表”的层面去理解它们。 但是在写作时，也就是进行信息输出的时候，我们必须去分析知识的“深层结构”，观察和调用知识与知识之间的深层关联，不然我们无法自如地将它们组织起来。因为一篇文章要被人读懂、要把人说服，需要缜密的思维、清晰的表达和详实的依据，这些都要求我们队知识的编码和组织达到一个相对较高的水平才行。</p></blockquote>

<p>自己下来多理解一下才行，开始写！</p>

<h4>作者论文的主要贡献</h4>

<ol>
<li>提出了一种找Tor出口节点使用的DNS服务器的方法</li>
<li>提出了名为DefecTor攻击方法，该方法属于关联攻击（correlation attack）的一种，用于溯源</li>
<li>在TorPS上分析了该新攻击方法对Tor用户造成的影响</li>
<li>提出了一种更好的衡量（evaluate）关联攻击的方法</li>
</ol>


<p><strong>本篇内容主要讲贡献2</strong>，后面逐步更新</p>

<h4>Tor DNS解析过程</h4>

<p>首先Tor的DNS都是从Tor网络中走的，这点是Tor client （web browser）预设值，目的是防止DNS外泄。因为Tor不能够代理UDP流量，所以Tor proxy将DNS request 包装进Tor CELL中。 浏览器将解析地址传给Tor client proxy， 然后Tor client 选择一个可以用的出口节点，并与其建立连接（应该是3跳连接，不是1跳连接，参考Tor spec）, Tor出口节点解析后，才会与目地网站建立TCP连接，成功之后返回Relay_connected cell. 另外一种DNS解析方法是构建Relay_resolve cell, 这个不同在于没有后续的TCP连接。</p>

<p>Tor出口节点会根据其主机配置进行DNS解析，主机的DNS解析一般流程 参考这里：<a href="http://blog.csdn.net/wy5761/article/details/19485761">Linux DNS</a></p>

<p>作者在文章中指出：大约4成的DNS流量都去了谷歌的DNS(8.8.8.8), 第二多的就是去本地DNS服务器解析。</p>

<h4>Defctor 攻击模型</h4>

<p>文章中的重要假设：</p>

<ol>
<li>每次网页访问只产生一个DNS请求，不考虑嵌套请求（embedded request）和 DNS caching</li>
<li>出口节点的DNS请求和正常用Tor browser 但不经过Tor 网络访问网站的DNS请求一样（不太清楚）</li>
<li>为了达到最好的关联攻击效果，用户选择的路径（出入口）被攻击者所控制。</li>
</ol>


<p><img src="http://i1-news.softpedia-static.com/images/news2/if-it-wanted-google-could-deanonymize-a-large-number-of-tor-connections-508863-3.png" alt="image" /></p>

<p>如上图，攻击者的位置有两个（需要同时满足）：</p>

<p>1）出口节点和DNS服务器之间（或者攻击者控制了DNS服务器）</p>

<p>2）在用户或者Tor guard之间（或者控制了Tor guard）</p>

<p>关联攻击，同时对比1）和2）处的流量，达到溯源目的是这篇文章的主要思想。不同的是该文章对比的是DNS流量，传统的方法是对比TCP流量。</p>

<h4>实验方法</h4>

<p>作者在论文中对：1）用户使用Tor访问哪些网站，2）多久访问一次这些网站，进行了统计，3）如何对Tor出口节点的DNS缓存进行了考虑。发现了如下结论：</p>

<p><strong>1）用户访问的网站服从幂律分布</strong></p>

<p><strong>2）作者实验使用的Tor出口节点大概每5分钟发出102次DNS请求（中位数），然后作者根据Tor带宽进行推测，真实网络中Tor出口节点大概每5分钟发出119.3次DNS请求（中位数），一次网站访问大概会平均造成10.3到不同域名的DNS请求，因此得出每十分钟大概可以发现23.2个不同网站。</strong></p>

<p><strong>3）Tor client 会缓存DNS请求，TTL最小为60s,最大为30分钟（目前统一为60s）。作者利用60s的滑动窗口，就可以发现所有的DNS请求，无论它是否缓存过。</strong></p>

<p>作者使用的Tor浏览器来发送DNS请求，但不使流量通过Tor网络，其目的在于为了防止原来有些顶会所讨论的Tor拒绝服务情况。作者意图通过DNS来判断用户所访问的网站，实现了一个Naive classifer, 主要通过<strong>unique domains: domains that are unique to a particular website</strong>来进行关联。 作者通过对DNS请求进行特征提取（extractdns），主要提取了observed domains, TTLs and IP-addresses这三个特征。然后利用dns2site（Go脚本代码）将生成的dns list map 到 具体的 site（具体算法是利用投票进行的）。</p>

<p>这里总结一下，Naive classifer 主要分类依据是<strong>unique domains mapping</strong>, 作者使用的数据集是top one million alexa websites 每个主页访问5次的那个数据集。</p>

<p>作者另外还根据DNS请求进行了额外的特征提取，从而可以利用Tao Wang 的 KNN算法进行分类。因此在这个实验中，作者利用了Tao Wang论文中所提出的一些特征，共1225个，然后利用Wa-KNN进行权重训练，最后进行分类。 我感觉作者分了两次来做实验，一次是在出口节点搜集DNS流量，第二次是在入口节点根据Tor log信息（log incoming and outgoing cells）来搜集流量，因此入口节点处采用的是KNN，出口节点采用的是Naive classifer。</p>

<p>这里再总结一下，KNN算法依据的是Wang et al.，作者使用的数据集是1000个（每个100个instances）网页， 和额外的100000个网页（每个1个instance）的数据集。 验证采用的是10折交叉验证方法。</p>

<p>那么问题来了，<strong>作者如何是将这两个结果关联起来的呢？</strong> 作者在论文中还提到了一个叫做 <em>hp（high precision）</em> 的attack，这个attack应该就是起到关联作用的。 只有当<em>hp attack</em>成功观察到入口和出口节点有相同网站时，才进行汇报（关联），相当的simple和effective。 在这里因为Naive classifer采用的DNS mapping, 其效果能达到 94.7%的recall和98.4%的precision，因此作者有理由认为，如果我在入口节点看到了一个我检查的网页，并且我在出口处也看到了该网站，那么就有较大的可能性达到了溯源的目的。然而，作者在出口节点观察到的DNS请求并不一定来自该入口节点，有可能别的用户也在同一时间从相同出口处访问了这个网站， 这就有个概率比例问题了（攻击者能观察多少比例的出口流量？）。果然，作者后来又说: We conclude that DefecTor attacks are perfectly precise for <strong>unpopular sites</strong>（因为popular site 会有很多人访问） because it is <strong>unlikely that more than one person is browsing a monitored site within the timeframe determined by the window length</strong>. 这个感觉还是有点道理的，看读者怎么想了吧。</p>

<h4>分析作者搜集DNS流量的实验平台框架（偏工程）</h4>

<p>作者搜集Alexa前100万的DNS流量，无论是从数量级还是工程难度来说都是具有一定挑战的。笔者饶有兴趣的看了一下大神们的做法，将一些要点记录在<a href="http://jasonzhuo.com/GolangRelated/">这里</a>.</p>

<h4>Refs</h4>

<p>[1] https://nymity.ch/tor-dns/</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>PF_ring and Jnetpcap</title>
      <link href="http://jason-zhuo.github.io//pfring-and-jnetpcap/"/>
      <pubDate>2016-01-21T00:00:00-06:00</pubDate>
      <author></author>
      <guid>http://jason-zhuo.github.io//pfring-and-jnetpcap</guid>
      <content:encoded><![CDATA[<h3>基于Jnetpcap使用Pf_ring（PF_ring and Jnetpcap）</h3>

<hr />

<p><img src="/assets/images/Pfringjnetpcap.png" alt="image" />
基于Jnetpcap的Pfring使用，以及网络数据包处理流程机制分析。</p>

<!-- more -->


<ol>
<li>last update 2016.3.5  修正l句子不通顺的地方。</li>
</ol>


<h2>1.PF ring 简介</h2>

<p>PF_RING是Luca Deri发明的提高内核处理数据包效率的网络数据包捕获程序，如Libpcap和TCPDUMP等。PF_RING是一种新型的网络socket，它可以极大的改进包捕获的速度。</p>

<h4>1.1 本文中的一些术语</h4>

<p><strong>NAPI:</strong>  NAPI是Linux新的网卡数据处理API，NAPI是综合中断方式与轮询方式的技术，NAPI 在高负载的情况下可以产生更好的性能，它避免了为每个传入的帧都产生中断。参考这个链接<a href="http://blog.csdn.net/zhangskd/article/details/21627963">NAPI</a></p>

<p><strong>Zero copy (ZC):</strong> 简单一点来说，零拷贝就是一种避免 CPU 将数据从一块存储拷贝到另外一块存储的技术。</p>

<p><strong>NPU:</strong>网络处理器</p>

<p><strong>DMA</strong>:直接内存存取, Direct Memory Access, 它允许不同速度的硬件装置来沟通，而不需要依赖于 CPU 的大量中断负载。</p>

<p><strong>Linux 网络栈</strong>：如下图所示，它简单地为用户空间的应用程序提供了一种访问内核网络子系统的方法。
<img src="https://www.ibm.com/developerworks/cn/linux/l-linux-networking-stack/figure2.gif" alt="enter image description here" /></p>

<h2>2. Libpcap抓包原理</h2>

<p>Libpcap的包捕获机制就是在数据链路层加一个旁路处理。当一个数据包到达网络接口时，libpcap首先利用已经创建的Socket从链路层驱动程序中获得该数据包的拷贝，再通过Tap函数将数据包发给BPF过滤器。BPF过滤器根据用户已经定义好的过滤规则对数据包进行逐一匹配，匹配成功则放入内核缓冲区(一次拷贝)，并传递给用户缓冲区（又一次拷贝），匹配失败则直接丢弃。如果没有设置过滤规则，所有数据包都将放入内核缓冲区，并传递给用户层缓冲区。</p>

<p><img src="/assets/images/Libpcap.jpg" alt="image" /></p>

<p>高速复杂网络环境下libpcap丢包的原因主要有以下两个方面：</p>

<ol>
<li>Cpu处于频繁中断状态，造成接收数据包效率低下。</li>
<li>数据包被多次拷贝，浪费了大量时间和资源。从网卡驱动到内核，再从内核到用户空间。</li>
</ol>


<h4>为啥用Pfring呢</h4>

<p>随着信息技术的发展，1 Gbit/s，10 Gbit/s 以及 100 Gbit/s 的网络会越来越普及，那么零拷贝技术也会变得越来越普及，这是因为网络链接的处理能力比 CPU 的处理能力的增长要快得多。高速网络环境下， CPU 就有可能需要花费几乎所有的时间去拷贝要传输的数据，而没有能力再去做别的事情，这就产生了性能瓶颈。</p>

<h2>3.PF_RING驱动家族</h2>

<p>这些驱动（在PF_RING/driver/PF_RING-aware中可用）设计用于提高数据包捕获，它把
数据包直接放入到PF_RING中，不经过标准Linux数据包分发机制。</p>

<h3>3.1 DNA</h3>

<p>对于那些希望在CPU利用率为0%（拷贝包到主机）情况下需要最大数据包捕获速度的用户来说，可以使用DNA(Direct NIC Access)驱动，它允许数据直接从网络接口上读取，它以零拷贝的方式同时绕过Linux 内核和 PF_RING模块。</p>

<p>左边这个图解释：<strong>Vanilla PF_RING</strong> 从NIC上通过Linux NAPI获取数据包拷贝。拷贝到PFring 环状缓存空间。然后用户空间的应用会从环状缓存空间读取数据包。从图中可以看出<strong>Vanilla PF_RING</strong> 有两次polling操作，一次从NIC到环状缓存空间（Linux 内核里面），另外一次从环状缓存空间到用户程序。</p>

<p>左边这个图，相对于传统的方式来说。首先Application使用的是mmap方式，较标准版本的Libpcap效率更高。Libpcap标准版是目前使用最多的用于从内核拷贝数据包到用户层的库，而libpcap-mmap是libpcap的一个改进版本。传统libpcap使用固定大小的存储缓冲器和保持缓冲器来完成数据包从内核到用户层的传递，而libpcap-mmap设计了一个大小可以配置的循环缓冲器，允许用户程序和内核程序同时对该循环缓冲器的不同数据区域进行直接的读取。其次，PF_ring使用的是NAPI，而不是Libpcap中使用的DMA方式调用系统函数netif_rx()将数据包从网卡拷贝到内核缓存。</p>

<p>右边这个图解释：<strong>DNA</strong>模式下通过NIC 的NPU(网络处理器)拷贝数据包到网卡上的缓存空间。然后直接通过DMA方式到用户层，同时绕过了PF_RING模块和Linux 内核缓存。官网已经证明了DNA方式能有更快的处理速率<a href="http://www.ntop.org/products/packet-capture/pf_ring/">Performance</a>。</p>

<p><img src="http://img.blog.csdn.net/20140622013115968?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9nMjUw/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="DNAandNAPI" /></p>

<p>让人多少有点遗憾的是，该方式并不是可以随意使用的，根据其License，提供免费下载，但是以二进制形式提供测试版本的库（也就是说使用5分钟或者达到一定包的处理数量之后软件就停了），如果需要长期使用，需要购买解锁的代码。</p>

<blockquote><p>ERROR: You do not seem to have a valid DNA license for eth0 [Intel 1 Gbit e1000e family].
We're now working in demo mode with packet capture
and transmission limited to 0 day(s) 00:05:00</p></blockquote>

<p>下图展示的是传统数据发送的整个过程，图片引用自<a href="http://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy1/">ibm.com</a>。
<img src="http://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy1/image001.jpg" alt="传统使用 read 和 write 系统调用的数据传输" /></p>

<h3>3.2 PF_RING-aware (ZC support)</h3>

<p>这个模块在路径 PF_RING/driver/PF_RING-aware下（带有zc后缀）。根据官方手册介绍，An interface in ZC mode provides the same performance as DNA. Zero Copy(ZC)。ZC和DNA实际上都是绕过Linux 内核和 PF_RING模块，这些模式下Linux内核将看不到任何数据包。</p>

<p><img src="http://i2.wp.com/www.ntop.org/wp-content/uploads/2011/08/pfring_mod.png?w=80%25" alt="PFring" /></p>

<h3>3.3 ZC</h3>

<p>PF ring 还有一个ZC模块。什么ZC，DNA，pfring-aware是有点乱，不好分清楚。ZC模块可以看做是DNA的后续版本（“It can be considered as the successor of DNA/LibZero that offers a single and consistent API implementing simple building blocks (queue, worker and pool) that can be used from threads, applications and virtual machines.”）。PF_RING ZC comes with a <strong>new generation of PF_RING aware drivers</strong>。感觉这个ZC新模块与DNA差别不大。</p>

<h2>4. Libpfring and Libpcap and Jnetpcap</h2>

<p>怎么让我们的其他应用程序使用pfring的高速特性呢？官方文档中说，Legacy statically-linked pcap-based applications need to be recompiled against the new PF_RING-enabled libpcap.a in order to take advantage of PF_RING. Do not expect to use PF_RING without recompiling your existing application. 也就是说需要和 PF_RING-enabled的 libpcap.a 进行重新编译应用程序才能够使用pfring的高速特性。</p>

<p>项目运用中，我打算使用JAVA程序来写一个高速的网络数据包处理程序。由于JAVA采用的是Jnetpcap，Jnetpcap是Libpcap的封装。而原本Libpcap本来不是支持Pfring的。因此，如果要用Java调用Pfring，就必须采用支持Pfring的Libpcap，而不是原本安装的Libpcap。所以，需要把原来的Libpcap卸载，安装Pfring的Libpcap。</p>

<pre><code>rmp -qa libpcap //查看Libpcap
rpm -e libpcap //删除Libpcap
</code></pre>

<p>Pfring 的安装过程可以参考这篇文章：<a href="http://www.cnblogs.com/tswcypy/p/3941619.html">Pfring安装</a></p>

<p>实验所用的服务器是Dell R730，em1,em2是万兆网卡，em3是千兆网卡
先卸载原来的网卡驱动，加载DNA驱动到em2网卡后是这样的：</p>

<pre><code>[root@localhost ~]# ethtool -i em2
driver: ixgbe
version: 4.1.5
firmware-version: 0x800004cf, 15.0.28
bus-info: 0000:01:00.1
supports-statistics: yes
supports-test: yes
supports-eeprom-access: yes
supports-register-dump: yes
supports-priv-flags: no
</code></pre>

<pre><code>[root@localhost ~]# ethtool -i em3
driver: igb
version: 5.2.13-k
firmware-version: 1.67, 0x80000b89, 15.0.28
bus-info: 0000:06:00.0
supports-statistics: yes
supports-test: yes
supports-eeprom-access: yes
supports-register-dump: yes
supports-priv-flags: no
</code></pre>

<p>运行我写的基于Jnetpcap的Java程序后，读取的硬件信息是这样的：</p>

<ol>
<li>flags=6, addresses=[[addr=[10], mask=[10], broadcast=null, dstaddr=null], [addr=[INET4:192.168.1.30], mask=[INET4:255.255.255.0], broadcast=[INET4:192.168.1.255], dstaddr=null], [addr=[17], mask=null, broadcast=[17], dstaddr=null]], name=em3, desc=<strong>PF_RING</strong></li>
<li>flags=6, addresses=[[addr=[17], mask=null, broadcast=[17], dstaddr=null]], name=em2, desc=<strong>PF_RING ZC/DNA</strong></li>
<li>flags=6, addresses=[[addr=[17], mask=null, broadcast=[17], dstaddr=null]], name=em1, desc=<strong>PF_RING ZC/DNA</strong></li>
</ol>


<p>可以看到，em1,em2由于加载了Pfring的特殊驱动，其描述已经变成了<strong>PF_RING ZC/DNA</strong>，em3由于还是用的原来的驱动，所以只有Pfring。</p>

<p>使用Java调用<strong>PF_RING ZC/DNA</strong>，如果没有Lisence, 仍然会出现刚才所讲的错误信息。
需要注意的是调用ZC的时候需要将原来：</p>

<p><code>Pcap.openlive(device.getName())</code></p>

<p>改为</p>

<p><code>Pcap.openlive(“ZC:”+device.getName())</code></p>

<p>奇怪的是，我用tcpreplay 最高速发送：1479130个包的时候
jnetpcap接收到16655个包之后就没有了，至今不知道原因。
recv=0, drop=0, ifdrop=0
recv=16655, drop=0, ifdrop=0
recv=16655, drop=0, ifdrop=0
recv=16655, drop=0, ifdrop=0</p>

<p>Jnetpcap不使用ZC/DNA特殊驱动的时候，仅用Jnetpcap+ Pfring模块，在高速网络环境下，我们发现丢包仍然很严重。
实验中，我们将em1和em2直接串联，使用Tcpreplay结果：</p>

<pre><code>Actual: 1479130 packets (812472290 bytes) sent in 4.06 seconds.Rated: 200116320.0 bps, 1526.77 Mbps, 364317.72 pps

Statistics for network device: em1
Attempted packets:         1479130
Successful packets:        1479130
Failed packets:            0
Retried packets (ENOBUFS): 0
Retried packets (EAGAIN):  0
</code></pre>

<p>使用Jnetpcap抓包结果：</p>

<pre><code>    recv=93179, drop=139931, ifdrop=0
        recv=201895, drop=721505, ifdrop=0
        recv=338379, drop=736488, ifdrop=0
        recv=490772, drop=736488, ifdrop=0
        recv=660812, drop=736488, ifdrop=0
        recv=663681, drop=736488, ifdrop=0
        recv=663681, drop=736488, ifdrop=0
</code></pre>

<p>可以发现，仅用Jnetpcap+ Pfring模块，在高速网络环境下，我们发现丢包仍然很严重。达到了50%。</p>

<h3>4.1Pf ring 工作模式</h3>

<p>PF_RING有3中工作模式：   pf_ring有三种透明模式（transparent_mode）。为0时走的是Linux标准的NAPI包处理流程；    为1时，包既走Linux标准包处理流程，也copy给pf_ring一份； 为2时，驱动只将包拷贝给pf_ring，内核则不会接收到这些包，1和2模式需要pf_ring特殊的网卡驱动的支持。NOTE from other website：</p>

<ul>
<li>默认为transparent=0，数据包通过标准的linux接口接收。任何驱动都可以使用该模式</li>
<li>transparent=1（使用于vanila和PF_RING-aware驱动程序），数据包分别拷贝到PF_RING和标准linux网络协议栈各一份</li>
<li>transparent=2（PF_RING-aware驱动程序），数据包近拷贝到PF_RING,而不会拷贝到标准的linux网络协议栈（tcpdump不会看到任何数据包）。</li>
<li>不要同时使用模式1和模式2到vanila驱动，否则将会抓到任何数据包。

<h3>4.2 Pf ring 包过滤</h3>

<p>pf_ring 普通模式下支持传统的BPF过滤器，由于DNA模式下，不再使用NAPI Poll，所以PF_RING的数据包过滤功能就不支持了，目前可以使用硬件层的数据包过滤，但只有intel的 82599网卡支持。Jnetpcap只能在pf_ring 普通模式下工作，因此只能够用BPF过滤器。</p></li>
</ul>


<blockquote><p>Written with <a href="https://stackedit.io/">StackEdit</a>.</p></blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>Dieharder and STS</title>
      <link href="http://jason-zhuo.github.io//dieharder-and-sts/"/>
      <pubDate>2015-10-10T00:00:00-05:00</pubDate>
      <author></author>
      <guid>http://jason-zhuo.github.io//dieharder-and-sts</guid>
      <content:encoded><![CDATA[<h3>Random or not? Dieharder and STS</h3>

<p><img src="http://i.stack.imgur.com/12L2d.gif" alt="image" />
Dieharder以及STS 套件使用 coauthored by Zihao Li, Jason Zhuo</p>

<!-- more -->


<h3>1. 介绍</h3>

<p>Dieharder是一个功能强大的软件，它是用来测试一序列伪随机数的随机性能的测试套件。
Dieharder主要测试rng（随机数产生器）的性能，它里面整合了一部分随机数产生器。其大致原理还是对随机数序列进行测试，但是其过程是先选中发生器，然后由发生器产生一序列的随机数，最后对发生器进行评估。不过Dieharder允许产生器产生较大的随机数序列。Dieharder在第三版中增添了<strong>全套</strong>的sts对于随机数测试的逻辑，但是并没有使用其源代码。基本操作方面，Dieharder网页上面的介绍只提及了如何测试内置发生器性能的操作，对于其他并没有过多提及。基本操作比较简洁，但是不是特别友好，需要输入的命令必须要特别熟悉才能够在不打开操作列表（dieharder -l）和内置发生器列表(dieharder -g -1)，同时操作时命令行的格式并没有规定，不确定要如何输入，相关操作方面还在摸索。（如-d和-g的位置在某些情况下需要变换，此处并没有介绍）Sts功能较为单一，只是对随机数序列进行测试并评估其随机程度。但其步骤较为清晰且容易操作。简单来说，如果只是为了将加密后的数据包进行分析，无疑Sts是较为干脆的。这里不确定数据包加密算法是否可以在Dieharder上测试，因为不确定加密算法算不算是一个随机数发生器。如果是为了检测数据包是否被加密过，则Sts是一个好的选择。</p>

<h3>2.基本操作演示</h3>

<p>使用Sts的话，首先安装sts，然后在终端定位到其目录。输入：</p>

<blockquote><p>./assess number</p></blockquote>

<p>可以开始测试，此处number是单个流的大小。接着输入0选择输入文件，然后写入文件的目录加载文件，如（data/000）。在检测算法列表中选择相应算法，此处填1可以全选，0则选择某几个。某些算法会让使用者设置块大小，这个是在检测时会用到的参数，可以选择使用缺省值。然后输入你准备测试的流的数目，其大小在开始时就已设置，选中你所读入文件的储存格式，如AscII和二进制保存。然后在experience/Algorithm Testingz中的相应文件查看测试结果。
对于Dieharder，可以先输入</p>

<blockquote><p>dieharder -l</p></blockquote>

<p>出现基本功能列表（dieharder所有的检测算法），-d 15便是测试内置发生器。</p>

<p><img src="/Users/zhuozhongliu/JasonzhuoGithubIO/assets/images/2015-10-10.png" alt="dieharder -l" /></p>

<p>接着可以输入dieharder -g -1显示所有的发生器。</p>

<p><img src="/Users/zhuozhongliu/JasonzhuoGithubIO/assets/images/2015-10-10-2.png" alt="dieharder -g" />
假如准备对第41进行测试，则输入dieharder -d 15 -g 41 -t 1000000,其中-t是选择发生器所产生序列的大小。</p>

<p>Dieharder操作外部产生器产生的随机数流操作，在此详述：先载入到文件目录，对于文件分为AscII码和二进制存储分别对待。</p>

<blockquote><p>使用dieharder -g 201 -f 文件名 -a，然后会输出结果；</p></blockquote>

<p>对于AscII码文件，需输入</p>

<blockquote><p>dieharder -g 202 -f 文件名 -a</p></blockquote>

<p>同时，这个文件的头部应做适当修改，否则无法读取。（注：对于二进制文件测试命令 dieharder –g 201 –f 文件名 –a,此处的-a指的是将运行所有的算法来检测文件，如果希望指定某一种算法时，可以使用dieharder –g 201 –f 文件名 –d 数字，此方法对于AscII同样适用）</p>

<p>头部修改举例：</p>

<blockquote><p>generator mt19937_1999 seed = 1274511046</p>

<p>type: d</p>

<p>count: 100000</p>

<p>numbit: 32</p>

<p>3129711816</p>

<p>85411969</p>

<p>2545911541</p></blockquote>

<p><img src="/Users/zhuozhongliu/JasonzhuoGithubIO/assets/images/2015-10-10-3.png" alt="dieharder -g 201 -f BBS.dat –a" /></p>

<h3>两者对比</h3>

<table>
<thead>
<tr>
<th> Name        </th>
<th style="text-align:center;">针对目标          </th>
<th style="text-align:center;">得到结果 </th>
<th style="text-align:center;">文件大小 </th>
<th style="text-align:center;"> 操作步骤</th>
<th style="text-align:center;">是否友好</th>
<th style="text-align:center;"> 对数据随机性的严格程度</th>
</tr>
</thead>
<tbody>
<tr>
<td>STS     </td>
<td style="text-align:center;"> 待检测其随机性的序列     </td>
<td style="text-align:center;">   较为详细，有明确的中间过程量 </td>
<td style="text-align:center;">  在处理大容量文件时速度一般</td>
<td style="text-align:center;">步骤多但明确，且有详细注释</td>
<td style="text-align:center;">操作较为友好</td>
<td style="text-align:center;">较为严格</td>
</tr>
<tr>
<td> Dieharder</td>
<td style="text-align:center;"> 待验证其产生序列是否随机的随机数产生器</td>
<td style="text-align:center;">只有一个P-value结果，且没有中间过程量</td>
<td style="text-align:center;">面向文件容量较大，速度较为优秀</td>
<td style="text-align:center;"> 输入命令较为令人费解，且需要对各操作目录比较熟悉，虽然简洁但不简单</td>
<td style="text-align:center;">操作步骤比较不友好，需要对软件有一定程度的理解</td>
<td style="text-align:center;">比较宽松</td>
</tr>
</tbody>
</table>


<h3>后记操作过程sts要容易一些，该软件更方便新手操作。dieharder更为简洁，将所有操作凝练在一行，使用者要对该软件有一定程度的熟悉。如此一来，利用dieharder进行测试也是较为方便的，不过dieharder没有sts那样可以查找到中间量的数据，而且暂时还没有找到如何控制dieharder进行部分文件读入的操作。（补充：方法已找到，在后面直接接-t 数字即可，比如dieharder –g 201 –f BBS.dat –d 101 –t 10000.此处的意思是使用-g 201来读入BBS.dat文件，同时使用 -d 101来进行测试，其中只读取10000位）</h3>

<p>dieharder无疑是更强大的，暂时使用有些生涩，不过其内含算法要比sts多，如果希望向更深处探究无疑dieharder更好一些。
使用两种软件对BBS.dat中等长的部分文件进行测试，就P-value而言，sts得出的值要略小一些。dieharder并没有展示其计算逻辑，但是应该也是对随机数进行测试，不过应该会对结果进行分析得出针对发生器的P-value。其网站上并不认同单独拿出一段数据测试其随机性是很好的手段，其认为应该从发生器的角度去思考，同时它对发生器的态度比较宽容，在P-value比较小的时候（小于0.01只是视为weak，小于0.01%才会被拒绝）它也承认这个发生器是合理的，而且weak级别的发生器也可以产生正常的随机数。所以单独就数据随机性检测而言，个人认为sts要严格一些。</p>

<h4>Useful site :</h4>

<p><a href="https://www.phy.duke.edu/~rgb/General/dieharder.php">Dieharder home page</a></p>

<p><a href="https://www.random.org/">Random.org</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Web Crawler Crawl4j</title>
      <link href="http://jason-zhuo.github.io//web-crawler-crawl4j/"/>
      <pubDate>2015-07-12T00:00:00-05:00</pubDate>
      <author></author>
      <guid>http://jason-zhuo.github.io//web-crawler-crawl4j</guid>
      <content:encoded><![CDATA[<h3>基于Crawl4j的网页爬虫（Web crawler based on Crawl4j）</h3>

<hr />

<p><img src="/assets/images/2015-07-12webcrawler.png" alt="image" />
基于Crawl4j的网页爬虫简单实现以及运用。</p>

<!-- more -->


<h3>1.起因</h3>

<p>最近老师叫我去下载一个“核武器”，但是这个核武器一共有400G大小，包括2万多个目录，10多万个文件。如果一个一个去下载，明显是不靠谱的。网上也有人放出了这些文件的BT文件，可惜BT文件太大(23MB)，我电脑一打开就卡顿得不行。后来打开了，但是下载进度一直卡在0.0%，最快速度也就20KB/s。老师又叮嘱<strong>一定</strong>要下载下来，于是我就停下了手上论文的工作，开始解决这个问题。一开始我没想太多，因为最近事情确实有点多，白天的事情忙不完，晚上还有兼职工作... 后来花了周六一下午时间，问题解决了,或多或少有些收获，又想到这篇blog或许不仅可以帮助他人学习进步，而且看到牛博士下载开源代码逐个目录的下载，很是辛苦。于是就又了这篇Blog.</p>

<p>另外，关于这个“核武器”，我就简单介绍一下，一开始我也是不知道的，后来老师通知我才晓得。</p>

<blockquote><p>可以说这次事件和斯诺登事件的影响力是不相上下的，但HT(Hacking Team)被黑不光光是让公众知道有这回事，随之而来还有整整415G的泄漏资料！里面有Flash 0day, Windows字体0day, iOS enterprise backdoor app, Android selinux exploit, WP8 trojan等等核武级的漏洞和工具。</p></blockquote>

<h3>2.方案思路</h3>

<p>既然迅雷BT下载速度奇慢无比，那就只好用基本HTTP去下载公布在网络上的镜像文件了。如果像牛博士那样一条条链接去另存为的话，肯定不行。之前看过通过Wget做全站镜像的命令</p>

<blockquote><p>wget -mk -c http://target.com</p></blockquote>

<p>这个也是解决方法之一，不过https网站不行，要出错。另外，上述命令在国内环境中下载速度慢，而且无法在远程VPS上进行下载（硬盘大小有限）。容易造成 <em>wget memory exhausted</em> 的错误，真是醉了。</p>

<p>另外一个解决方法就是利用网络爬虫将所有的文件链接获取，然后逐一发起HTTP请求去下载。有个叫做<strong>plowdown</strong>的命令可以解决，参考<a href="https://github.com/mcrapet/plowshare">plowshare</a>
安装完之后，利用
<code>plowdown --fallback -m Links.txt</code>
就行了。</p>

<p>后期，打算采用迅雷离线空间来下载上述链接，估计会比较快一点，唯一不足的就是没有文件目录结构，后期按照目录来下载估计就行了。</p>

<h3>3.Crawl4J简介</h3>

<p>Crawl4J是一个开源的Java爬虫程序，总共才三十多个类，比较简单，非常适合爬虫入门的学习。</p>

<p>下载地址：<a href="https://github.com/yasserg/crawler4j">https://github.com/yasserg/crawler4j</a></p>

<p>下载crawler4j-4.1-jar-with-dependencies.jar，然后将其导入外部依赖库。</p>

<h3>4. 源代码</h3>

<p>Mycrawler主要对每条链接进行处理。Controller里面设置了Java使用代理（链接到远程VPS）。代码是仿照官网给出的实例改编的，具体方法说明参考Crawl4j说明文档。</p>

<h4>4.1.Mycrawler class</h4>

<pre><code class="">import edu.uci.ics.crawler4j.crawler.Page;
import edu.uci.ics.crawler4j.crawler.WebCrawler;
import edu.uci.ics.crawler4j.parser.HtmlParseData;
import edu.uci.ics.crawler4j.url.WebURL;

import java.io.BufferedWriter;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.util.Set;

public class Mycrawler extends WebCrawler {
    static double totalFilesCount = 0.0;
    static  File file = new File("Links.txt");
    public  static  void writelinkstoFile(String url){
        try {
            BufferedWriter bf  = new BufferedWriter(new FileWriter(file,true));
            bf.write(url+"\n");
            bf.flush();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    /**
     * This method receives two parameters. The first parameter is the page
     * in which we have discovered this new url and the second parameter is
     * the new url. You should implement this function to specify whether
     * the given url should be crawled or not (based on your crawling logic).
     * In this example, we are instructing the crawler to ignore urls that
     * have css, js, git, ... extensions and to only accept urls that start
     * with "http://www.ics.uci.edu/". In this case, we didn't need the
     * referringPage parameter to make the decision.
     */
    public boolean checkifIsDIR(String href){
        if (href.endsWith("/")){
            return true;
        }else
        return false;

    }
    @Override
    public boolean shouldVisit(Page referringPage, WebURL url) {
        String orghref =url.getURL();
        String href = url.getURL().toLowerCase();

        boolean iswithinDomain  =href.startsWith(Controller.TargetSite.toLowerCase());
        if (iswithinDomain&amp;&amp;checkifIsDIR(href)){
          //  System.out.println(href);
            return true;
        }else if (iswithinDomain){
            {
                if(href.equalsIgnoreCase("http://hacking.technology/Hacked%20Team/Exploit_Delivery_Network_android.tar.gz")){
                    return false;
                }
                if(href.equalsIgnoreCase("http://hacking.technology/Hacked%20Team/Exploit_Delivery_Network_windows.tar.gz")){
                    return false;
                }
                if(href.equalsIgnoreCase("http://hacking.technology/Hacked%20Team/support.hackingteam.com.tar.gz")){
                    return false;
                }

            }
            writelinkstoFile(orghref);
            totalFilesCount++;
        }
        return false;

    }

    /**
     * This function is called when a page is fetched and ready
     * to be processed by your program.
     */
    @Override
    public void visit(Page page) {
        String url = page.getWebURL().getURL();
        System.out.println("visited URL: " + url);

        if (page.getParseData() instanceof HtmlParseData) {
            HtmlParseData htmlParseData = (HtmlParseData) page.getParseData();
            String text = htmlParseData.getText();
            String html = htmlParseData.getHtml();
            Set&lt;WebURL&gt; links = htmlParseData.getOutgoingUrls();

            System.out.println("Text length: " + text.length());
            System.out.println("Html length: " + html.length());
            System.out.println("Number of outgoing links: " + links.size());
        }
    }
}
</code></pre>

<h4>4.2.Controller class</h4>

<pre><code>import edu.uci.ics.crawler4j.crawler.CrawlConfig;
import edu.uci.ics.crawler4j.crawler.CrawlController;
import edu.uci.ics.crawler4j.fetcher.PageFetcher;
import edu.uci.ics.crawler4j.robotstxt.RobotstxtConfig;
import edu.uci.ics.crawler4j.robotstxt.RobotstxtServer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


import java.util.Properties;

/**
 * Created by jasonzhuo on 2015/7/11.
 */
public class Controller {
    public  static String TargetSite= "http://hacking.technology/Hacked%20Team/";
    private static final Logger logger = LoggerFactory.getLogger(Controller.class);

    static int numberOfCrawlers = 7;

    public static void main(String[] args) {

        CrawlConfig crawlConfig = new CrawlConfig();
        crawlConfig.setCrawlStorageFolder("I:\\HTools");

        //proxy setup
       //crawlConfig.setProxyHost("127.0.0.1");
      //  crawlConfig.setProxyPort(1080);
        Properties prop = System.getProperties();
        prop.setProperty("socksProxyHost", "127.0.0.1");
        prop.setProperty("socksProxyPort", "1080");
        crawlConfig.setMaxDownloadSize(Integer.MAX_VALUE);
        crawlConfig.setMaxOutgoingLinksToFollow(Integer.MAX_VALUE);
        crawlConfig.setMaxDepthOfCrawling(32767);
        crawlConfig.setMaxPagesToFetch(Integer.MAX_VALUE);
        System.out.println(crawlConfig.toString());
        PageFetcher pageFetcher = new PageFetcher(crawlConfig);
        RobotstxtConfig robotstxtConfig = new RobotstxtConfig();
        RobotstxtServer robotstxtServer = new RobotstxtServer(robotstxtConfig, pageFetcher);
        try {
            CrawlController controller = new CrawlController(crawlConfig, pageFetcher, robotstxtServer);
            //controller.addSeed("http://www.ics.uci.edu/~welling/");
            controller.addSeed(TargetSite);
            // controller.addSeed("https://www.google.com.hk/");
            controller.start(Mycrawler.class, numberOfCrawlers);
            System.out.println("Recorded total number of files :"+Mycrawler.totalFilesCount);
        } catch (Exception e) {
            e.printStackTrace();
        }


    }


}
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>Random forest clustering</title>
      <link href="http://jason-zhuo.github.io//random-forest-clustering/"/>
      <pubDate>2015-05-23T00:00:00-05:00</pubDate>
      <author></author>
      <guid>http://jason-zhuo.github.io//random-forest-clustering</guid>
      <content:encoded><![CDATA[<h3>随机森林聚类（Random forest clustering）</h3>

<hr />

<p><img src="/assets/images/2015-05-23-randforest.png" alt="image" />
随机森林聚类分析</p>

<!-- more -->


<h3>1.起因</h3>

<p>网络上现存的很多资料都是关于随机森林的分类和回归分析，但很少有材料讲解随机森林的聚类分析过程。
随机森林以及随机森林的分类回归过程我就不做详细介绍了，你可以参考网络上的其他资料 [1]，[2], [3], [8]。</p>

<p>分析随机森林聚类算法还有如下的原因：</p>

<blockquote><p>GBDT和随机森林在ESL书里面说是目前最优的两种算法</p></blockquote>

<p>然后呢，最近也在研究一些新的聚类算法，就顺带研究一些随机森林是如何用了聚类的吧。</p>

<h3>2.随机森林聚类</h3>

<p>随机森林聚类算法的最核心思想：</p>

<p><strong>1. 将非监督学习转换为监督学习</strong></p>

<p><strong>2. 相似度矩阵的计算</strong></p>

<p>合成数据集（synthetic dataset）中包括的样本是通过样本合成算法产生新的样本的集合。最常用的就是合成算法就是生成和原始数据集（Orignial dataset）样本数量一致，相同特征的集合。</p>

<p>举个网上的例子[7]来说吧：
假设有两个特征\(x_1\),\(x_2\).其中，\(x_1\)是连续变量服从\(Normal(0,1)\),\(x_2\)是0，1分布。一种合成数据的方法是根据每个特征的分布进行估计，值得注意的是随机生成的数据是各自独立的。另外一种合成方法是根据样本的概率进行随机选择，例如在样本中\(P(male)=0.4\),\(P(female)=0.6\)，那么合成的\(x_1\)的值就从原始数据集里面随机选择一个，合成的\(x_2\)的值就按照上述概率生成0或1。</p>

<p>新的训练样本集合多了一个新的特征，该特征用来区分该样本是合成的还是原始的，用二进制表示即可。</p>

<blockquote><p>新的训练样本集合（多一个类标签特征）=合成样本数据集合+原始数据集合</p></blockquote>

<h4>2.1相似度矩阵</h4>

<p>随机森林在建模的同时，还提供了样本相似性度量，即相似度矩阵(简记为 Prox 矩阵)。当用一棵树对所有数据进行判别时，这些数据最终都将达到该树的某个叶节点上.可以用两个样本在每棵树的同一个节点上出现的频率大小，来衡量这两个样本之间的相似程度，或两个样本属于同一类的概率大小。</p>

<p>Prox矩阵\(P=p_{ij} \)生成过程如下：</p>

<ol>
<li>对于样本数为N的训练集合，首先生成一个\(N\times N\)的矩阵，\(p_{ii}=1\)，其他元素为0。</li>
<li>对于任意两个样本\(x_i\),\(x_j\),若他们出现在生成的同一棵树上的同一个叶节点上，则$$p[i][j]= 1+p[i][j]$$</li>
<li>重复上述过程直到m棵树全部生成，得到相应的矩阵</li>
<li>进行归一化处理$$p[i][j]= \frac{p[i][j]}{N}$$</li>
</ol>


<p>在随机森林进行聚类的过程中，运用到到了两个样本之间的相似度进行计算。我们知道在聚类算法中存在很多衡量两个样本之间距离的方法，欧几里得距离等等。但是在随机森林中的距离是如下计算的[5]:</p>

<p>$$DISSIM[i][j]= \sqrt{1-p[i][j]}$$</p>

<h4>2.2随机森林聚类计算过程</h4>

<p>随机森林聚类算法：</p>

<ol>
<li>生成合成样本集合</li>
<li>生成新的带类新标签的训练集合N，样本只包括两类：合成的数据，原始的数据</li>
<li>根据N生成相似度矩阵P</li>
<li>将P中被我们标记为合成数据的行和列删除</li>
<li>运用其他聚类算法进行聚类</li>
</ol>


<p>为啥我们要生产合成数据集，然后还要将其加入原始训练集合呢？我直观地认为直接在原始数据集上算相似性不就ok啦。后来我反应过来，但是原始数据是不带类标签的（非监督学习）。产生合成数据集的目的就是和原始数据进行类标记，将非监督学习转换为监督学习。</p>

<p>步骤5中居然采用还是其他的聚类算法，只不过替换了距离测量方法，这点感觉有点那个啥<img src="/assets/smilies/30.gif" id="similey">。</p>

<h4>2.3随机森林聚类应用</h4>

<p>文献[3]和文献[4]主要是用随机森林对医疗疾病属性的聚类，文献[3][4]用的是PAM（Partitioning around medoids）聚类算法。
随机森林的Prox矩阵可以作为多个基于不相似度聚类算法的输入，例如文献[5]中同样利用的是K-Medoids聚类算法PAM。作者将其应用到网络流量的聚类上，还比较创新。</p>

<h4>2.4随机森林聚类Case study</h4>

<p>利用R语言中的函数<code>rfClustering(model,noClusters=4)</code>进行聚类分析，这个例子中没有体现出合成数据集合的生成的，因为iris数据集是带类标签的。</p>

<ol>
<li><code>set&lt;-iris</code> 载入iris数据集，iris数据集是带类标签的</li>
<li><code>md&lt;-CoreModel(Species ~., set,model="rf",rfNoTrees=30)</code> 利用随机森林得到模型，模型md中包括了相似度矩阵</li>
<li><code>mdCluster&lt;-rfClustering(md,3)</code>聚成3类</li>
</ol>


<p><code>rfClustering</code>这个函数根据作者介绍，内在是调用的PAM聚类算法的。</p>

<p>聚类结果：</p>

<p><img src="/assets/images/2015-05-23rfresult.png" alt="image" /></p>

<h3>参考文献</h3>

<p>[1]<a href="http://www.cnblogs.com/leftnoteasy/archive/2011/03/07/random-forest-and-gbdt.html">http://www.cnblogs.com/leftnoteasy/archive/2011/03/07/random-forest-and-gbdt.html</a></p>

<p>[2]<a href="http://blog.sciencenet.cn/blog-661364-615921.html">http://blog.sciencenet.cn/blog-661364-615921.html</a></p>

<p>[3]<a href="http://blog.csdn.net/songzitea/article/details/10035757">http://blog.csdn.net/songzitea/article/details/10035757</a></p>

<p>[4]Shi T, Seligson D, Belldegrun A S, et al. Tumor classification by tissue microarray profiling: random forest clustering applied to renal cell carcinoma[J]. Modern Pathology, 2005, 18(4): 547-557.</p>

<p>[5]Shi T, Horvath S. Unsupervised learning with random forest predictors[J]. Journal of Computational and Graphical Statistics, 2006, 15(1).</p>

<p>[6]Wang Y, Xiang Y, Zhang J. Network traffic clustering using Random Forest proximities[C]//Communications (ICC), 2013 IEEE International Conference on. IEEE, 2013: 2058-2062.</p>

<p>[7]<a href="http://stats.stackexchange.com/questions/92725/unsupervised-random-forest-using-weka">http://stats.stackexchange.com/questions/92725/unsupervised-random-forest-using-weka</a></p>

<p>[8]<a href="http://www.zilhua.com/629.html">http://www.zilhua.com/629.html</a></p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
