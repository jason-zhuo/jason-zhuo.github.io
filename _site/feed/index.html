<?xml version="1.0" encoding="utf-8"?>
  <rss version="2.0"
        xmlns:content="http://purl.org/rss/1.0/modules/content/"
        xmlns:atom="http://www.w3.org/2005/Atom"
  >
  <channel>
    <title>Jaosonzhuo's blog</title>
    <link href="http://jasonzhuo.com//feed/" rel="self" />
    <link href="http://jasonzhuo.com/" />
    <lastBuildDate>2015-04-26T02:12:25+08:00</lastBuildDate>
    <webMaster>jason_zhuo@iclould.com</webMaster>
    
    <item>
      <title>Shadowsocks in Lab</title>
      <link href="http://jasonzhuo.com//shadowsocks-in-lab/"/>
      <pubDate>2015-04-25T00:00:00+08:00</pubDate>
      <author></author>
      <guid>http://jasonzhuo.com//shadowsocks-in-lab</guid>
      <content:encoded><![CDATA[<p><img src="https://dn-teddysun.qbox.me/wp-content/uploads/2015/shadowsocks_logo.png" alt="image" />
好多童鞋搞不清楚我在实验室搭建的Shadowsocks如何使用，在这里给你们科普一下，知道原理之后使用起来就更加顺心了，保证一口气科研就到天亮哦~</p>

<!-- more -->


<h4>注明：以下高能内容转载自<a href="http://vc2tea.com/whats-shadowsocks/">vc2tea的博客</a></h4>

<p>在很久很久以前，我们访问各种网站都是简单而直接的，用户的请求通过互联网发送到服务提供方，服务提供方直接将信息反馈给用户.</p>

<p><img src="http://vc2tea.com/public/upload/whats-shadowsocks-01.png" alt="image" /></p>

<p>然后有一天，GFW 就出现了，他像一个收过路费的强盗一样夹在了在用户和服务之间，每当用户需要获取信息，都经过了 GFW，GFW将它不喜欢的内容统统过滤掉，于是客户当触发 GFW 的过滤规则的时候，就会收到 Connection Reset 这样的响应内容，而无法接收到正常的内容.</p>

<p><img src="http://vc2tea.com/public/upload/whats-shadowsocks-02.png" alt="image" /></p>

<p>聪明的人们想到了利用境外服务器代理的方法来绕过 GFW 的过滤，其中包含了各种HTTP代理服务、Socks服务、VPN服务, <strong>Tor</strong>, <strong>Freegate</strong> … 其中以 ssh tunnel 的方法比较有代表性</p>

<ol>
<li>首先用户和境外服务器基于 ssh 建立起一条加密的通道</li>
<li>用户通过建立起的隧道进行代理，通过 ssh server 向真实的服务发起请求</li>
<li>服务通过 ssh server，再通过创建好的隧道返回给用户</li>
</ol>


<p><img src="http://vc2tea.com/public/upload/whats-shadowsocks-03.png" alt="image" /></p>

<p>由于 ssh 本身就是基于 RSA 加密技术，所以 GFW 无法从数据传输的过程中的加密数据内容进行关键词分析，避免了被重置链接的问题，但由于创建隧道和数据传输的过程中，ssh 本身的特征是明显的，所以 GFW 一度通过分析连接的特征进行干扰，导致 ssh 存在被定向进行干扰的问题。</p>

<p>这时候<strong>shadowsocks</strong>横空出世。简单看来，shadowsocks是将原来 ssh 创建的 Socks5 协议拆开成 server 端和 client 端，所以下面这个原理图基本上和利用 ssh tunnel 大致类似。</p>

<h4>shadowsocks工作原理</h4>

<p>1、6) 客户端发出的请求基于 Socks5 协议跟 ss-local 端进行通讯，由于这个 ss-local 一般是本机或路由器或局域网的其他机器，不经过 GFW，所以解决了上面被 GFW 通过特征分析进行干扰的问题</p>

<p>2、5) ss-local 和 ss-server 两端通过多种可选的加密方法进行通讯，经过 GFW 的时候是常规的TCP包，没有明显的特征码而且 GFW 也无法对通讯数据进行解密</p>

<p>3、4) ss-server 将收到的加密数据进行解密，还原原来的请求，再发送到用户需要访问的服务，获取响应原路返回</p>

<p><img src="http://vc2tea.com/public/upload/whats-shadowsocks-04.png" alt="image" /></p>

<hr />

<p>截止目前，笔者所使用最流畅、最稳定的就是shadowsocks，一口气翻墙不费劲，以后再也不用操心翻墙了。传送门：<a href="https://github.com/shadowsocks/shadowsocks">shadowsocks下载地址</a></p>

<p>在我们实验室中，翻墙代理设置可以为网关的IP地址192.168.1.1，也可以为本地127.0.0.1地址。唯一区别就是上图中SS Local代理程序运行的位置不同，（刘博士懂了撒~）。在实验室环境中，童鞋们可以自己设置利用本机的代理，也可以使用阿江搭建在网关上的SS Local代理，效果一致。没有在实验室的童鞋就只能靠自己本地代理啦。最后使用的时候需要设置为Socks5代理类型就可以正常使用了。
<img src="/assets/smilies/37.gif" id="similey">
(css inline 表情，深夜特别鸣谢刘博士)
<img src="/assets/smilies/28.gif" id="similey"></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Botnet and fast flux</title>
      <link href="http://jasonzhuo.com//Botnet%20and%20fast%20flux/"/>
      <pubDate>2015-04-25T00:00:00+08:00</pubDate>
      <author></author>
      <guid>http://jasonzhuo.com//Botnet and fast flux</guid>
      <content:encoded><![CDATA[<p>Botnet and fast flux 特征介绍；Botminer论文初看</p>

<!-- more -->


<h2>Botnet and fast flux</h2>

<hr />

<h3>几个定义</h3>

<dl>
<dt><strong>Round-Robin DNS (RRDNS) </strong>* </dt>
<dd>Round-robin的意思是循环的意思，顾名思义，这种DNS返回的A记录不只一个，返回的记录是一个列表，列表里面记录的先后顺序是循环出现的。</dd>
<dt><strong>Content Distribution Networks (CDN)</strong></dt>
<dd>中文名叫内容分发网络，和RRDNS比较类似，不过返回的记录的TTL值相对RRDNS更低。CDN系统能够实时地根据网络流量和各节点的连接、负载状况以及到用户的距离和响应时间等综合信息将用户的请求重新导向离用户最近的服务节点上。其目的是使用户可就近取得所需内容，解决 Internet网络拥挤的状况，提高用户访问网站的响应速度。</dd>
<dt><strong><em>Fast-Flux Service Networks (FFSN)</em></strong></dt>
<dd>FFSN更加创新地利用了RRDNS和CDN的上述特性，来降低恶意服务器被发现和关闭的概率。FFSN的特点下面会讨论。</dd>
</dl>

<hr />

<h3>Fast flux特征</h3>

<ol>
<li><strong>不重复的IP地址数量</strong>：通常情况来说合法DNS查询不重复的IP地址为1~3个，而fast flux查询结果中会有5~6个，以确保至少有一个IP可以连接。</li>
<li><strong>NS数量</strong>：NS数量是指在单一次DNS 查询中所得到的NS （Name server）数量。客户端与DNS 主机进行查询时,可能透过快速变动网域技术掩护DNS 主机,因此NS Records 与NS 的A Records 可能有多笔记录,相较之下,合法的FQDN 其NS Records 与NS 的A Records比较少。</li>
<li><strong>ASN数量与注册时间</strong>：指对ASN进行查询时，主机使用的IP所属的ASN是否属于同一个单位。由于CDN主机使用的IP所属的ASN多属于同一个单位，而fast flux主机大多分散在世界各地，与CDN向比较之下，主机使用的IP所属的ASN属于不同单位。注册时间能够缩小选取范围。</li>
<li><strong>Domain age</strong>:指合法网站记录的TTL时间相对于恶意网站更长；恶意网站的FQDN与对应的IP记录不会长时间存留电脑，电脑必须时常进行DNS查询，以更新记录。RFC1912建议TTL最小为1~5天，这么长！而FFSN的TTL值一般小于600秒。但是一般不使用TTL值来判定FFSN，因为这样的误报率比较高，合法使用的CDN也会返回比较低的TTL值。因此TTL值一般用来区分FFSN/CDN和RRDNS.</li>
</ol>


<h4>常见僵尸网络检测程序</h4>

<table>
<thead>
<tr>
<th> Name        </th>
<th style="text-align:center;">TYPE                       </th>
<th style="text-align:center;">Protocols </th>
</tr>
</thead>
<tbody>
<tr>
<td>BotSniffer     </td>
<td style="text-align:center;"> centralized servers     </td>
<td style="text-align:center;">   IRC,HTTP </td>
</tr>
<tr>
<td> BotHunter      </td>
<td style="text-align:center;"> structure independent  </td>
<td style="text-align:center;">Protocol independent </td>
</tr>
<tr>
<td> Botminer </td>
<td style="text-align:center;"> structure independent  </td>
<td style="text-align:center;">    Protocol independent       </td>
</tr>
</tbody>
</table>


<h3>BotMiner 初看</h3>

<h5>BotMiner的系统架构</h5>

<p><img src="/assets/images/2015-04-19-botminer.png" alt="Structure of Botminer" /></p>

<h4>论文中的一些定义</h4>

<dl>
<dt><strong>A平面</strong></dt>
<dd>主要检测恶意行为模型,文章用Snort检测某主机的行为,主机的扫描行为用的是Bothunter的SCADE，下载地址：<a href="http://www.bothunter.net/">Cyber-TA.BotHunter Free Internet Distribution Page, 2008. </a>。文章采用两种异常检测模式，一种是高频率异常扫描次数，另外一种是带加权的失败连接次数。文章作者为了检测恶意邮件，开发了一种</dd>
<dt><strong>C平面</strong></dt>
<dd>网络通信关系模型和流特征模型</dd>
<dt><strong>AC跨平面关联</strong></dt>
<dd>发现AC平面之间的某种关联关系，僵尸网络评分s(h)</dd>
<dt><strong>C平面的聚类</strong></dt>
<dd>  定义 C-flow 为一段时间E内具有相同源IP，目的IP和目的端口的网络流</dd>
</dl>

<p><strong>C-flow的一些特征</strong></p>

<ol>
<li>FPH: the number of flows per hour.</li>
<li>PPF: the number of packets per flow</li>
<li>BPP: the average number of bytes per packets</li>
<li>BPS: the average number of bytes per second</li>
</ol>


<h4>参考文献</h4>

<p><a href="https://www.usenix.org/legacy/event/sec08/tech/full_papers/gu/gu_html/">[1]BotMiner: Clustering Analysis of Network Traffic for
Protocol- and Structure-Independent Botnet Detection</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Structure Risk Minimization,SRM</title>
      <link href="http://jasonzhuo.com//structure-risk-minimizationsrm/"/>
      <pubDate>2015-04-18T00:00:00+08:00</pubDate>
      <author></author>
      <guid>http://jasonzhuo.com//structure-risk-minimizationsrm</guid>
      <content:encoded><![CDATA[<p><img src="/assets/images/2015-04-18-StatML.png" alt="image" />
本周学术交流，张老师给我们介绍了结构风险最小化原理，这篇博客对交流内容进行了精炼和总结。</p>

<!-- more -->


<h2>结构风险最小化</h2>

<hr />

<p>基于数据的机器学习有2个方法，第1个是经典的统计估计方法，通过训练样本来估计参数值，代表是R.A Fisher 的统计理论（线性回归）。但我们经常做的预测真的<strong>靠谱</strong>吗？答案是，不靠谱，因为我们在做预测的时候，引入了一个很强的假设条件———样本集要满足独立同分布条件（iid-independent identically distribulted）。</p>

<p>第2个方法是V.Vapnik等人提出的统计学习理论。该理论也被称作VC理论，最重要的概念就是VC维。VC维~函数复杂度~是一个正整数（也叫做函数容量）。函数复杂度越大，预测效果越差。</p>

<p>$$R_{exp}=\int L(y,f)p(x,y)dxdy \leq \frac{1}{m} \sum_i^{m} {(y_i-f(x_i))}^2 +\psi(\frac{V(f)}{m})$$</p>

<p>其中\(V(f)\)就是函数VC维，m为样本个数。上式表明在样本一定的情况下，VC维越高，期望风险越大。</p>

<p>统计学习理论-结构风险最小化SRM:<br/>
        $$期望风险&lt;=经验风险 + 置信度$$</p>

<p>这里简单介绍一下Vladimir Vapnik。Vladimir Vapnik是俄国人，1936年出生。1964年，他于莫斯科的控制科学学院获得博士学位。毕业后，他一直在该校工作直到1990年，这期间他作为不多（估计在国内可能有点那个），但是1991加入贝尔实验室之后，他在1995年发明了SVM。从此，他就出名了，2006年当上了美国科学院院士。
<img src="/assets/smilies/16.gif" alt="image" /></p>

<blockquote><p>所谓的结构风险最小化就是在保证分类精度（经验风险）的同时，降低学习机器的 VC 维，可以使学习机器在整个样本集上的期望风险得到控制。</p></blockquote>

<p>见图：
<img src="http://img.my.csdn.net/uploads/201105/29/0_1306659245j5ZS.gif" alt="image" /></p>

<p>张老师随后介绍了SVM中最著名的核方法。将低维度转化到高维度，计算复杂度却和低维度差不多。这太牛了，佩服Mercer。张老师补充到，这个是1911年提出来的（当时清王朝刚被推翻，国内可能无人潜心科研吧），之后Mercer的论文成为了睡美人，直到后来被Vapnik发现。</p>

<p><strong>核函数</strong></p>

<p>$$&lt;\psi(x),\psi(y)>=k(x,y)$$</p>

<p>核函数的作用</p>

<p>$$R^{n} \to H^{\infty}$$
(H为Hilbert空间)</p>

<p>非线性问题<strong>一定</strong>能够线性化</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Big data Short notes</title>
      <link href="http://jasonzhuo.com//big-data-course/"/>
      <pubDate>2015-04-18T00:00:00+08:00</pubDate>
      <author></author>
      <guid>http://jasonzhuo.com//big-data-course</guid>
      <content:encoded><![CDATA[<p>Bigdata培训课程，听了一天，感觉听不大懂，工程细节上的东西太多了，而且自己这方面也刚刚起步，因此本文就稍微记一下我比较感兴趣的内容。</p>

<!-- more -->


<h3>Spark Streaming</h3>

<blockquote><p>目前的大数据处理可以分为如下3个类型：</p></blockquote>

<ol>
<li>复杂的批量数据处理：10min~数小时</li>
<li>基于历史数据的交互式查询： 10sec~ 数分钟</li>
<li>基于实时数据流的数据处理（Streaming data processing): 数百毫秒到数秒</li>
</ol>


<p>除了Spark，流式计算计算系统比较有名的包括Twitter Storm和Yahoo S4。现在所提及的Storm主要是指Apache Storm ，Apache Storm的前身是 Twitter Storm 平台，目前已经归于 Apache 基金会管辖。Storm已经出现好多年了，而且自从2011年开始就在Twitter内部生产环境中使用，还有其他一些公司。而Spark Streaming是一个新的项目, 2013年开始。</p>

<p>Spark的流式计算还是要弱于Storm的，作者在<a href="http://www.csdn.net/article/2014-08-04/2821018">这篇文章中</a>认为互联网公司对于Storm的部署还是多于Spark。这篇文章对<a href="http://blog.csdn.net/anzhsoft/article/details/38168025">流式计算</a>系统的设计考虑的一些要素进行了比较详细的讨论。这篇文章介绍了Storm和Streaming框架的<a href="http://www.open-open.com/lib/view/open1426129553435.html">对比</a>. 如此说来，Storm在以后的项目中估计要用到<img src="/assets/smilies/8.gif" alt="image" />.</p>

<p>相对与<strong>Mapreduce</strong>来说，Mapreduce的输入数据集合是静态的，不能动态变化。因此适合于离线处理。Mapreduce的使用场景包括，简单的网站pv,uv统计，搜索引擎建立索引，海量数据查找，复杂数据的分析和算法实现（聚类，分类，推荐，图算法等）</p>

<p><strong>Yarn</strong> 的提出，解决了多计算框架直接的数据无法共享问题，同时负责集群资源的统一管理和调度。</p>

<p>运行在YARN上的计算框架：
1. 离线计算框架Mapreduce
2. DAG计算框架Tez
3. 流式计算框架Strom
4. 内存计算框架Spark</p>

<blockquote><p>Written with <a href="https://stackedit.io/">StackEdit</a>.</p></blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>hello world again</title>
      <link href="http://jasonzhuo.com//hello-world-again/"/>
      <pubDate>2015-04-15T00:00:00+08:00</pubDate>
      <author></author>
      <guid>http://jasonzhuo.com//hello-world-again</guid>
      <content:encoded><![CDATA[<p>I'm back!</p>

<!-- more -->


<p>时隔了这么久没写过博客，感觉有点对不起起初建立博客的初衷。于是想把我的博客给恢复起来。今天尝试了向我的博客里面添加数学公式的方法：</p>

<h3>添加行内公式</h3>

<pre><code>\\(公式\\)
</code></pre>

<h3>添加行间公式</h3>

<pre><code>$$公式$$
</code></pre>

<p>例如行间公式：\(\sum_{i\to 2}^i\)</p>

<p>$$\sum_{i\to 2}^i$$</p>

<p>具体来说实现比较简单：
如果用Mou渲染Math公式，尝试在<strong>default.html</strong>加上如下js，表示让Mou去加载Mathjax的脚本</p>

<p><code>&lt;script type="text/javascript"
 src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"&gt;
&lt;/script&gt;</code></p>

<p>在试过上面的方法后，我还尝试了直接在mou编辑过程中加入上述代码：
<img src="/assets/images/2015-04-16-mathequa.png" alt="image" /></p>

<p>加入之后倒是可以一边编辑一边看到编辑结果，但是容易造成Mou在渲染过程中的卡顿，不推荐这样使用。</p>

<p>编辑完成之后，在上传之前可以进行检查，把上述图中的代码加入就可以进行检查了。</p>

<p>在此庆祝我的博客复活啦，以后就可以写写高大上的数学公式了~ 就这样了，碎觉碎觉，累死累活！</p>

<blockquote><h2>... in mathematics you didn't understand things, you just get used to them. --J.von.Newmann </h2></blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>Big data Algorithm Report</title>
      <link href="http://jasonzhuo.com//BigDatareport/"/>
      <pubDate>2014-12-01T00:00:00+08:00</pubDate>
      <author></author>
      <guid>http://jasonzhuo.com//BigDatareport</guid>
      <content:encoded><![CDATA[<p><img src="/assets/images/2014-12-1-Bigdata.jpg" alt="image" />
这篇主要讲的是周末听大数据讲座的听后感。</p>

<!-- more -->


<p>大数据时代，我们每个人都在谈论大数据，每个人都可以说自己在研究大数据。但是真正懂大数据的还是只有真正的大神们。大数据设计的概念太大了，就和云计算差不多，涵盖的面也非常广泛，可以说是上到天文下到地理，从宏观的天体运动到微观的分子结构，简直无所不包。</p>

<p>报告上半场是9点到12点，下半场是14点到18点。我听了上半场的上半部分和整个下半场。听完报告感觉都快虚脱了。。。各种听不懂，各种高大上~ 能听懂的都是前面几页PPT。真实台上一分钟，台下十年功啊！台上PPT的每一个图或者一个表，虽然只展示了不到1分钟，但其背后的付出都是可想而知的。我还发现大牛们的一个普遍规律，就是讲着讲着就冒一句英语，而且语速还不慢。简直让我们摸不着头脑。</p>

<h4>大数据的4V特点:</h4>

<p><img src="/assets/images/2014-12-1-big4.jpg" alt="image" /></p>

<p>这四个特点好像很多老师来做报告都要讲，不过都是简单地提了一下。</p>

<h4>报告简单回忆</h4>

<h5>1.Collective attention and Collective allocation</h5>

<p>报告首先是由沈华伟老师给我们带来的Collective attention and Collective allocation. Collective attention主要是衡量一篇论文到底能获得多大关注度。Collective allocation是说的有些诺贝尔颁奖的时候，有些情况是颁发给论文的第1作者，但也有些情况是颁发给论文的第3或者第4作者。这个资源分配的问题就是Collective allocation问题，其主要思想是结合每个作者在该领域的引用文章的影响进行合理分配。</p>

<h5>2. A query-based algorithm framework for dynamic data analysis</h5>

<p>刘兴武老师介绍了一下动态处理大规模数据的方法。主要思想是用基于查询的方法来处理动态数据。</p>

<h5>3.融合空间认知学的空间数据库研究</h5>

<p>邵杰老师介绍了如何利用空间认知学上的研究思路来进行数据研究。空间认知学包括了地理学和认知心理学。传统的寻路算法都是考虑的是最近邻，最短路径等，然而，现在我们需要寻找最易到达邻。这一点还是很有研究意义的。</p>

<h5>4.Learn to Hash for Big Data</h5>

<p>李武军老师介绍了如何用Hash函数来处理数据。在大数据的前提下，Hash函数可以降维，提高处理效率，节约存储空间。李老师主要介绍了监督Hash学习方法，非监督Hash学习和多模态Hash学习方法。Hash学习和最近邻检索有着密切关系。</p>

<h5>5.Big-data Machine Learning</h5>

<p>林智仁老师给我们带来的是关于分布式的机器学习方法。林老师的讲解深入浅出，相对容易理解一些。林老师还形象地描述了做大数据的人，形容的相当形象。</p>

<p><img src="/assets/images/2014-12-01-live.png" alt="image" /></p>

<blockquote><p>Big data is like teenage sex, everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it.</p></blockquote>

<p>传统的机器学习都是单机的，在处理大数据时效率很低，训练时间长，不能够让模型随着新增样本的变化而相应变化。考虑提高效率的方法，有（1）买一个超大RAM （2）Disk-level 机器学习 （3）GPU计算 （4）分布式机器学习。但是林老师也说了，不是所有的都可以用分布式来解决问题。分布式还会带来很多问题，例如同步，通信时间。原来的Ph.D学生都主要研究训练计算时间如何提高上去了，很少有人研究过在大数据情况下，如何快速载入大数据。</p>

<h5>5.用物理观点看图挖掘问题</h5>

<p>好不容易到了最后，本以为周涛老师会以幽默风趣的谈吐和深入浅出的讲解结束本次报告。但是，恰恰相反，到了很多人已经筋疲力尽的时候，周涛老师讲了他是如何用物理学上的方法解决网络链路预测问题的。他所提出的模型似乎很强大，比原来的牛文章里面的还要好。不过我没听懂，PPT上各种公式弄得我晕头转向。但是，周涛老师提到了用统计物理的方法来研究计算机问题，挺有启发的。</p>

<h5>6.秘书问题与在线算法</h5>

<p>孙晓明老师介绍了秘书问题和在线算法相关工作。在通常情况下，数据都是一个接着一个到达的，正如秘书来面试一样，是一个接着一个和面试官面试的。面试官如何在这种情况下选择最优的秘书，是在线算法需要解决的工作。秘书问题是多对一的问题，online matching是多对多的问题。这些问题有关动态分配和动态取最优解，比较有意思。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Matlab2014b Install</title>
      <link href="http://jasonzhuo.com//matlab2014b-install/"/>
      <pubDate>2014-11-18T00:00:00+08:00</pubDate>
      <author></author>
      <guid>http://jasonzhuo.com//matlab2014b-install</guid>
      <content:encoded><![CDATA[<p><img src="/assets/images/2014-11-18-matlab.png" alt="image" />
本文描述了作者安装matlab2014b的折腾手记，本着折腾自己方便他人的思想，体现了作者乐于助人的中心思想。</p>

<!-- more -->


<p>距离上一次写博客都好久了，于是本文出于为了不忘记如何写博客以及就很多小伙伴的问题进行统一解答的目的，写下了折腾手记。本文不鼓励大家使用盗版软件。支持正版，人人有责，本文作者在XBG童鞋的诱惑下，已经在app store上购买了超过300元的正版软件。</p>

<blockquote><p>使用正版就是免去了折腾的麻烦，珍爱生命，请用正版。——本文作者</p></blockquote>

<p>好多用Mac的童鞋在升级最新的操作系统之后，就无法使用Matlab了，这为我们科研造成了一些小麻烦。最新的Matlab2014b可以在Yosemite上面运行。特别感谢破解人员的辛勤劳动，我这里就补充一些细枝末节的东西，以解答小伙伴们在MAC平台下安装过程中的一些问题。</p>

<p>首先给出<a href="http://bbs.feng.com/read-htm-tid-8467093.html">Matlab2014b全部平台的下载链接</a></p>

<h4>问题：</h4>

<ol>
<li>作者好不容易下载完成，然后解压运行，发现matlab mac版本提示powerpc应用程序不再被支持。</li>
<li>载入镜像安装，输入密钥后发现产品列表里面只有几个基本插件，其他功能都不在了</li>
</ol>


<h4>解决办法：</h4>

<ol>
<li>下载的iso文件千万别用the unarchiever解压，右键iso文件，注意是iso不是解压出来的那个图标，用DiskImageMounter打开。<img src="/assets/images/2014-11-18-matlab1.png" alt="image" /></li>
<li>使用UltraISO编辑下载的matlab的ISO文件，<a href="http://www.downxia.com/downinfo/659.html">UltraISO下载</a></li>
</ol>


<p>用UltraISO打开matlab的ISO文件:</p>

<p><img src="/assets/images/2014-11-18-matlab3.png" alt="image" /></p>

<p>进入matlab的ISO文件目录java/jar/，删除install.jar</p>

<p><img src="/assets/images/2014-11-18-matlab4.png" alt="image" /></p>

<p>再添加破解的install.jar</p>

<p><img src="/assets/images/2014-11-18-matlab5.png" alt="image" /></p>

<p>最后点保存
<img src="/assets/images/2014-11-18-matlab6.png" alt="image" />
<img src="/assets/images/2014-11-18-matlab7.png" alt="image" /></p>

<p>断网后，再次用DiskImageMounter打开matlab安装程序，输入刚才使用的密钥：29797-39064-48306-32452。发现已经有很多功能了，进行正常安装需要（9GB+）。默认情况下我们用不到那么多工具包，为了节约磁盘空间，可以将一些不常用的，非本专业的工具包取消。（视个人情况而定,可以全部安装）</p>

<p>正常安装完成后，激活即可：
activation file = license.lic</p>

<p><img src="/assets/images/2014-11-18-matlab8.png" alt="image" />
<img src="/assets/images/2014-11-18-matlab9.png" alt="image" /></p>

<p>然后在应用程序中找到刚刚安装完成的Matlab.app，右键进入包内容，将/Applications/MATLAB_R2014b/bin/maci64/libmwservices.dylib 用破解的该文件进行替换即可</p>

<h4>最后就可以愉快地进行科研活动啦~！</h4>
]]></content:encoded>
    </item>
    
    <item>
      <title>Auto Blog Post</title>
      <link href="http://jasonzhuo.com//AutoBlogpost/"/>
      <pubDate>2014-10-19T00:00:00+08:00</pubDate>
      <author></author>
      <guid>http://jasonzhuo.com//AutoBlogpost</guid>
      <content:encoded><![CDATA[<p><img src="/assets/images/2014-10-19-bash_logo.jpg" alt="image" /></p>

<p>每次写完Blog要不是忘记该输什么命令，就是懒得一个一个命令行敲。为了省事，自己根据Bash语言写了一个自动发表已经撰写好的Blog。</p>

<!-- more -->


<p>每次写完Blog要不是忘记该输什么命令，就是懒得一个一个命令行敲。</p>

<blockquote><p>Shell脚本语言(Shell Script)，Shell脚本与Windows/Dos下的批处理相似，也就是用各类命令预先放入到一个文件中，方便一次性执行的一个程序文件，主要是方便管理员进行设置或者管理用的。但是它比Windows下的批处理更强大，比用其他编程程序编辑的程序效率更高，毕竟它使用了Linux/Unix下的命令。——百度百科</p></blockquote>

<p>Shell脚本语言为实现自动化提供了良好帮助，真不愧为胶水语言，确实省事了很多。
下面展示了我如何利用脚本语言来一键发表提交撰写好的博客内容。脚本内容比较简单，基本逻辑是先输入提交本次提交的评注，然后脚本会自动删除你在本地Git仓库中删除的文件，不然无法提交到远程Git库中。其中“--cached”表示只删除本地git仓库的文件内容，本地的文件还没有删除，所以--cached可以考虑去掉。最后3个命令基本上都和网上的教程差不多。代码虽然很简单，但是还是有一定作用。</p>

<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="lineno"> 1</span> <span class="nb">echo</span> -n <span class="s2">&quot;Enter comment:&quot;</span>
<span class="lineno"> 2</span> <span class="nb">read  </span>comment
<span class="lineno"> 3</span> <span class="nv">files</span><span class="o">=</span><span class="sb">`</span>git status <span class="p">|</span> grep deleted <span class="p">|</span> awk <span class="s1">&#39;{print \$2}&#39;</span><span class="sb">`</span>
<span class="lineno"> 4</span> <span class="k">if</span> <span class="o">[</span> <span class="s2">&quot;${files}&quot;</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
<span class="lineno"> 5</span>    <span class="nb">echo</span> <span class="s2">&quot;nothing to remove&quot;</span>
<span class="lineno"> 6</span>  <span class="k">else</span>
<span class="lineno"> 7</span>         <span class="k">for</span> var in <span class="nv">$files</span>
<span class="lineno"> 8</span>         <span class="k">do</span>
<span class="lineno"> 9</span>             <span class="c">#echo $var</span>
<span class="lineno">10</span>             git rm --cached <span class="nv">$var</span>
<span class="lineno">11</span>         <span class="k">done</span>
<span class="lineno">12</span> <span class="k">fi</span>
<span class="lineno">13</span> git add .
<span class="lineno">14</span> git commit -m <span class="s2">&quot;$comment&quot;</span>
<span class="lineno">15</span> git push origin master
<span class="lineno">16</span> <span class="nb">exit </span>0</code></pre></div>


<p>给予该脚本可执行权限后，直接执行./Publish.sh即可轻松提交到远程Git仓库啦~</p>

<div class="highlight"><pre><code class="language-bash" data-lang="bash">chmod +x Publish.sh
./Publish.sh</code></pre></div>


<p>效果图：</p>

<p><img src="/assets/images/2014-10-19-AutoblogRes.png" alt="image" /></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Hello world</title>
      <link href="http://jasonzhuo.com//hello-world/"/>
      <pubDate>2014-10-18T00:00:00+08:00</pubDate>
      <author></author>
      <guid>http://jasonzhuo.com//hello-world</guid>
      <content:encoded><![CDATA[<p><img src="/assets/images/2014-10-18-hello_world.jpg" alt="image" /></p>

<p>几经折腾，我的个人博客终于首次上线啦。我的博客主要记录我平时的一些想法和技术总结。在飞速发展的年代里，闭门造车会让人乐不思蜀，不识时务。因此，我希望把我的一些想法和技术和大家探讨。</p>

<!-- more -->


<p>早在之前就有搭建自己博客的想法，因为很多企业和学校在招聘的时候都提到了个人主页这一项。个人主页的确能够帮我们更好地展示自己。但由于自己怕麻烦，嫌太折腾一直没有时间来细心搭建自己的博客。现在，终于在NSTL实验室刘博士的耐心指导下顺利完成自己博客的搭建。在此特别鸣谢NSTL实验室刘博士。</p>

<p>我的博客还没有购买域名，不过我感觉我的博客地址还算不错啦，io结尾确实还挺新鲜的。以后再考虑购买域名这件事吧。</p>

<p>博客的主要目的还是出于技术交流，学术交流的目的，因为做学术如果缺少了交流，总感觉还没入门，更别提发表创新性学术论文了。博客同时也会偶尔记录一下生活中的美好经历，以免遗忘。</p>

<p>最后引用一下名言吧，以此勉励自己：</p>

<blockquote><p>"Let the wind of freedom blow 让自由之风劲吹"</p></blockquote>

<p>这句传自16世纪德国人类学家修顿的名言，自被斯坦福确立为校训之日起，一直鼓舞着学者大胆创新，勇于实现自己的想法。</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
