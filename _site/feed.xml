<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jaosonzhuo&#39;s blog</title>
    <description>Jasonzhuo&#39;s blog</description>
    <link>http://jasonzhuo.com/</link>
    <atom:link href="http://jasonzhuo.com//feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Dieharder and STS</title>
        <description>&lt;h3&gt;Random or not? Dieharder and STS&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/12L2d.gif&quot; alt=&quot;image&quot; /&gt;
Dieharder以及STS 套件使用 coauthored by Zihao Li, Jason Zhuo&lt;/p&gt;

&lt;!-- more --&gt;


&lt;h3&gt;1. 介绍&lt;/h3&gt;

&lt;p&gt;Dieharder是一个功能强大的软件，它是用来测试一序列伪随机数的随机性能的测试套件。
Dieharder主要测试rng（随机数产生器）的性能，它里面整合了一部分随机数产生器。其大致原理还是对随机数序列进行测试，但是其过程是先选中发生器，然后由发生器产生一序列的随机数，最后对发生器进行评估。不过Dieharder允许产生器产生较大的随机数序列。Dieharder在第三版中增添了&lt;strong&gt;全套&lt;/strong&gt;的sts对于随机数测试的逻辑，但是并没有使用其源代码。基本操作方面，Dieharder网页上面的介绍只提及了如何测试内置发生器性能的操作，对于其他并没有过多提及。基本操作比较简洁，但是不是特别友好，需要输入的命令必须要特别熟悉才能够在不打开操作列表（dieharder -l）和内置发生器列表(dieharder -g -1)，同时操作时命令行的格式并没有规定，不确定要如何输入，相关操作方面还在摸索。（如-d和-g的位置在某些情况下需要变换，此处并没有介绍）Sts功能较为单一，只是对随机数序列进行测试并评估其随机程度。但其步骤较为清晰且容易操作。简单来说，如果只是为了将加密后的数据包进行分析，无疑Sts是较为干脆的。这里不确定数据包加密算法是否可以在Dieharder上测试，因为不确定加密算法算不算是一个随机数发生器。如果是为了检测数据包是否被加密过，则Sts是一个好的选择。&lt;/p&gt;

&lt;h3&gt;2.基本操作演示&lt;/h3&gt;

&lt;p&gt;使用Sts的话，首先安装sts，然后在终端定位到其目录。输入：&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;./assess number&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;可以开始测试，此处number是单个流的大小。接着输入0选择输入文件，然后写入文件的目录加载文件，如（data/000）。在检测算法列表中选择相应算法，此处填1可以全选，0则选择某几个。某些算法会让使用者设置块大小，这个是在检测时会用到的参数，可以选择使用缺省值。然后输入你准备测试的流的数目，其大小在开始时就已设置，选中你所读入文件的储存格式，如AscII和二进制保存。然后在experience/Algorithm Testingz中的相应文件查看测试结果。
对于Dieharder，可以先输入&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;dieharder -l&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;出现基本功能列表（dieharder所有的检测算法），-d 15便是测试内置发生器。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/Users/zhuozhongliu/JasonzhuoGithubIO/assets/images/2015-10-10.png&quot; alt=&quot;dieharder -l&quot; /&gt;&lt;/p&gt;

&lt;p&gt;接着可以输入dieharder -g -1显示所有的发生器。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/Users/zhuozhongliu/JasonzhuoGithubIO/assets/images/2015-10-10-2.png&quot; alt=&quot;dieharder -g&quot; /&gt;
假如准备对第41进行测试，则输入dieharder -d 15 -g 41 -t 1000000,其中-t是选择发生器所产生序列的大小。&lt;/p&gt;

&lt;p&gt;Dieharder操作外部产生器产生的随机数流操作，在此详述：先载入到文件目录，对于文件分为AscII码和二进制存储分别对待。&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;使用dieharder -g 201 -f 文件名 -a，然后会输出结果；&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;对于AscII码文件，需输入&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;dieharder -g 202 -f 文件名 -a&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;同时，这个文件的头部应做适当修改，否则无法读取。（注：对于二进制文件测试命令 dieharder –g 201 –f 文件名 –a,此处的-a指的是将运行所有的算法来检测文件，如果希望指定某一种算法时，可以使用dieharder –g 201 –f 文件名 –d 数字，此方法对于AscII同样适用）&lt;/p&gt;

&lt;p&gt;头部修改举例：&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;generator mt19937_1999 seed = 1274511046&lt;/p&gt;

&lt;p&gt;type: d&lt;/p&gt;

&lt;p&gt;count: 100000&lt;/p&gt;

&lt;p&gt;numbit: 32&lt;/p&gt;

&lt;p&gt;3129711816&lt;/p&gt;

&lt;p&gt;85411969&lt;/p&gt;

&lt;p&gt;2545911541&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/Users/zhuozhongliu/JasonzhuoGithubIO/assets/images/2015-10-10-3.png&quot; alt=&quot;dieharder -g 201 -f BBS.dat –a&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;两者对比&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt; Name        &lt;/th&gt;
&lt;th style=&quot;text-align:center;&quot;&gt;针对目标          &lt;/th&gt;
&lt;th style=&quot;text-align:center;&quot;&gt;得到结果 &lt;/th&gt;
&lt;th style=&quot;text-align:center;&quot;&gt;文件大小 &lt;/th&gt;
&lt;th style=&quot;text-align:center;&quot;&gt; 操作步骤&lt;/th&gt;
&lt;th style=&quot;text-align:center;&quot;&gt;是否友好&lt;/th&gt;
&lt;th style=&quot;text-align:center;&quot;&gt; 对数据随机性的严格程度&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;STS     &lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt; 待检测其随机性的序列     &lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt;   较为详细，有明确的中间过程量 &lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt;  在处理大容量文件时速度一般&lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt;步骤多但明确，且有详细注释&lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt;操作较为友好&lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt;较为严格&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; Dieharder&lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt; 待验证其产生序列是否随机的随机数产生器&lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt;只有一个P-value结果，且没有中间过程量&lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt;面向文件容量较大，速度较为优秀&lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt; 输入命令较为令人费解，且需要对各操作目录比较熟悉，虽然简洁但不简单&lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt;操作步骤比较不友好，需要对软件有一定程度的理解&lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt;比较宽松&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;h3&gt;后记操作过程sts要容易一些，该软件更方便新手操作。dieharder更为简洁，将所有操作凝练在一行，使用者要对该软件有一定程度的熟悉。如此一来，利用dieharder进行测试也是较为方便的，不过dieharder没有sts那样可以查找到中间量的数据，而且暂时还没有找到如何控制dieharder进行部分文件读入的操作。（补充：方法已找到，在后面直接接-t 数字即可，比如dieharder –g 201 –f BBS.dat –d 101 –t 10000.此处的意思是使用-g 201来读入BBS.dat文件，同时使用 -d 101来进行测试，其中只读取10000位）&lt;/h3&gt;

&lt;p&gt;dieharder无疑是更强大的，暂时使用有些生涩，不过其内含算法要比sts多，如果希望向更深处探究无疑dieharder更好一些。
使用两种软件对BBS.dat中等长的部分文件进行测试，就P-value而言，sts得出的值要略小一些。dieharder并没有展示其计算逻辑，但是应该也是对随机数进行测试，不过应该会对结果进行分析得出针对发生器的P-value。其网站上并不认同单独拿出一段数据测试其随机性是很好的手段，其认为应该从发生器的角度去思考，同时它对发生器的态度比较宽容，在P-value比较小的时候（小于0.01只是视为weak，小于0.01%才会被拒绝）它也承认这个发生器是合理的，而且weak级别的发生器也可以产生正常的随机数。所以单独就数据随机性检测而言，个人认为sts要严格一些。&lt;/p&gt;

&lt;h4&gt;Useful site :&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.phy.duke.edu/~rgb/General/dieharder.php&quot;&gt;Dieharder home page&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.random.org/&quot;&gt;Random.org&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 10 Oct 2015 00:00:00 +0800</pubDate>
        <link>http://jasonzhuo.com///dieharder-and-sts/</link>
        <guid isPermaLink="true">http://jasonzhuo.com///dieharder-and-sts/</guid>
      </item>
    
      <item>
        <title>Web Crawler Crawl4j</title>
        <description>&lt;h3&gt;基于Crawl4j的网页爬虫（Web crawler based on Crawl4j）&lt;/h3&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2015-07-12webcrawler.png&quot; alt=&quot;image&quot; /&gt;
基于Crawl4j的网页爬虫简单实现以及运用。&lt;/p&gt;

&lt;!-- more --&gt;


&lt;h3&gt;1.起因&lt;/h3&gt;

&lt;p&gt;最近老师叫我去下载一个“核武器”，但是这个核武器一共有400G大小，包括2万多个目录，10多万个文件。如果一个一个去下载，明显是不靠谱的。网上也有人放出了这些文件的BT文件，可惜BT文件太大(23MB)，我电脑一打开就卡顿得不行。后来打开了，但是下载进度一直卡在0.0%，最快速度也就20KB/s。老师又叮嘱&lt;strong&gt;一定&lt;/strong&gt;要下载下来，于是我就停下了手上论文的工作，开始解决这个问题。一开始我没想太多，因为最近事情确实有点多，白天的事情忙不完，晚上还有兼职工作... 后来花了周六一下午时间，问题解决了,或多或少有些收获，又想到这篇blog或许不仅可以帮助他人学习进步，而且看到牛博士下载开源代码逐个目录的下载，很是辛苦。于是就又了这篇Blog.&lt;/p&gt;

&lt;p&gt;另外，关于这个“核武器”，我就简单介绍一下，一开始我也是不知道的，后来老师通知我才晓得。&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;可以说这次事件和斯诺登事件的影响力是不相上下的，但HT(Hacking Team)被黑不光光是让公众知道有这回事，随之而来还有整整415G的泄漏资料！里面有Flash 0day, Windows字体0day, iOS enterprise backdoor app, Android selinux exploit, WP8 trojan等等核武级的漏洞和工具。&lt;/p&gt;&lt;/blockquote&gt;

&lt;h3&gt;2.方案思路&lt;/h3&gt;

&lt;p&gt;既然迅雷BT下载速度奇慢无比，那就只好用基本HTTP去下载公布在网络上的镜像文件了。如果像牛博士那样一条条链接去另存为的话，肯定不行。之前看过通过Wget做全站镜像的命令&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;wget -mk -c http://target.com&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;这个也是解决方法之一，不过https网站不行，要出错。另外，上述命令在国内环境中下载速度慢，而且无法在远程VPS上进行下载（硬盘大小有限）。容易造成 &lt;em&gt;wget memory exhausted&lt;/em&gt; 的错误，真是醉了。&lt;/p&gt;

&lt;p&gt;另外一个解决方法就是利用网络爬虫将所有的文件链接获取，然后逐一发起HTTP请求去下载。有个叫做&lt;strong&gt;plowdown&lt;/strong&gt;的命令可以解决，参考&lt;a href=&quot;https://github.com/mcrapet/plowshare&quot;&gt;plowshare&lt;/a&gt;
安装完之后，利用
&lt;code&gt;plowdown --fallback -m Links.txt&lt;/code&gt;
就行了。&lt;/p&gt;

&lt;p&gt;后期，打算采用迅雷离线空间来下载上述链接，估计会比较快一点，唯一不足的就是没有文件目录结构，后期按照目录来下载估计就行了。&lt;/p&gt;

&lt;h3&gt;3.Crawl4J简介&lt;/h3&gt;

&lt;p&gt;Crawl4J是一个开源的Java爬虫程序，总共才三十多个类，比较简单，非常适合爬虫入门的学习。&lt;/p&gt;

&lt;p&gt;下载地址：&lt;a href=&quot;https://github.com/yasserg/crawler4j&quot;&gt;https://github.com/yasserg/crawler4j&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;下载crawler4j-4.1-jar-with-dependencies.jar，然后将其导入外部依赖库。&lt;/p&gt;

&lt;h3&gt;4. 源代码&lt;/h3&gt;

&lt;p&gt;Mycrawler主要对每条链接进行处理。Controller里面设置了Java使用代理（链接到远程VPS）。代码是仿照官网给出的实例改编的，具体方法说明参考Crawl4j说明文档。&lt;/p&gt;

&lt;h4&gt;4.1.Mycrawler class&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;import edu.uci.ics.crawler4j.crawler.Page;
import edu.uci.ics.crawler4j.crawler.WebCrawler;
import edu.uci.ics.crawler4j.parser.HtmlParseData;
import edu.uci.ics.crawler4j.url.WebURL;

import java.io.BufferedWriter;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.util.Set;

public class Mycrawler extends WebCrawler {
    static double totalFilesCount = 0.0;
    static  File file = new File(&quot;Links.txt&quot;);
    public  static  void writelinkstoFile(String url){
        try {
            BufferedWriter bf  = new BufferedWriter(new FileWriter(file,true));
            bf.write(url+&quot;\n&quot;);
            bf.flush();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    /**
     * This method receives two parameters. The first parameter is the page
     * in which we have discovered this new url and the second parameter is
     * the new url. You should implement this function to specify whether
     * the given url should be crawled or not (based on your crawling logic).
     * In this example, we are instructing the crawler to ignore urls that
     * have css, js, git, ... extensions and to only accept urls that start
     * with &quot;http://www.ics.uci.edu/&quot;. In this case, we didn&#39;t need the
     * referringPage parameter to make the decision.
     */
    public boolean checkifIsDIR(String href){
        if (href.endsWith(&quot;/&quot;)){
            return true;
        }else
        return false;

    }
    @Override
    public boolean shouldVisit(Page referringPage, WebURL url) {
        String orghref =url.getURL();
        String href = url.getURL().toLowerCase();

        boolean iswithinDomain  =href.startsWith(Controller.TargetSite.toLowerCase());
        if (iswithinDomain&amp;amp;&amp;amp;checkifIsDIR(href)){
          //  System.out.println(href);
            return true;
        }else if (iswithinDomain){
            {
                if(href.equalsIgnoreCase(&quot;http://hacking.technology/Hacked%20Team/Exploit_Delivery_Network_android.tar.gz&quot;)){
                    return false;
                }
                if(href.equalsIgnoreCase(&quot;http://hacking.technology/Hacked%20Team/Exploit_Delivery_Network_windows.tar.gz&quot;)){
                    return false;
                }
                if(href.equalsIgnoreCase(&quot;http://hacking.technology/Hacked%20Team/support.hackingteam.com.tar.gz&quot;)){
                    return false;
                }

            }
            writelinkstoFile(orghref);
            totalFilesCount++;
        }
        return false;

    }

    /**
     * This function is called when a page is fetched and ready
     * to be processed by your program.
     */
    @Override
    public void visit(Page page) {
        String url = page.getWebURL().getURL();
        System.out.println(&quot;visited URL: &quot; + url);

        if (page.getParseData() instanceof HtmlParseData) {
            HtmlParseData htmlParseData = (HtmlParseData) page.getParseData();
            String text = htmlParseData.getText();
            String html = htmlParseData.getHtml();
            Set&amp;lt;WebURL&amp;gt; links = htmlParseData.getOutgoingUrls();

            System.out.println(&quot;Text length: &quot; + text.length());
            System.out.println(&quot;Html length: &quot; + html.length());
            System.out.println(&quot;Number of outgoing links: &quot; + links.size());
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;4.2.Controller class&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;import edu.uci.ics.crawler4j.crawler.CrawlConfig;
import edu.uci.ics.crawler4j.crawler.CrawlController;
import edu.uci.ics.crawler4j.fetcher.PageFetcher;
import edu.uci.ics.crawler4j.robotstxt.RobotstxtConfig;
import edu.uci.ics.crawler4j.robotstxt.RobotstxtServer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


import java.util.Properties;

/**
 * Created by jasonzhuo on 2015/7/11.
 */
public class Controller {
    public  static String TargetSite= &quot;http://hacking.technology/Hacked%20Team/&quot;;
    private static final Logger logger = LoggerFactory.getLogger(Controller.class);

    static int numberOfCrawlers = 7;

    public static void main(String[] args) {

        CrawlConfig crawlConfig = new CrawlConfig();
        crawlConfig.setCrawlStorageFolder(&quot;I:\\HTools&quot;);

        //proxy setup
       //crawlConfig.setProxyHost(&quot;127.0.0.1&quot;);
      //  crawlConfig.setProxyPort(1080);
        Properties prop = System.getProperties();
        prop.setProperty(&quot;socksProxyHost&quot;, &quot;127.0.0.1&quot;);
        prop.setProperty(&quot;socksProxyPort&quot;, &quot;1080&quot;);
        crawlConfig.setMaxDownloadSize(Integer.MAX_VALUE);
        crawlConfig.setMaxOutgoingLinksToFollow(Integer.MAX_VALUE);
        crawlConfig.setMaxDepthOfCrawling(32767);
        crawlConfig.setMaxPagesToFetch(Integer.MAX_VALUE);
        System.out.println(crawlConfig.toString());
        PageFetcher pageFetcher = new PageFetcher(crawlConfig);
        RobotstxtConfig robotstxtConfig = new RobotstxtConfig();
        RobotstxtServer robotstxtServer = new RobotstxtServer(robotstxtConfig, pageFetcher);
        try {
            CrawlController controller = new CrawlController(crawlConfig, pageFetcher, robotstxtServer);
            //controller.addSeed(&quot;http://www.ics.uci.edu/~welling/&quot;);
            controller.addSeed(TargetSite);
            // controller.addSeed(&quot;https://www.google.com.hk/&quot;);
            controller.start(Mycrawler.class, numberOfCrawlers);
            System.out.println(&quot;Recorded total number of files :&quot;+Mycrawler.totalFilesCount);
        } catch (Exception e) {
            e.printStackTrace();
        }


    }


}
&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Sun, 12 Jul 2015 00:00:00 +0800</pubDate>
        <link>http://jasonzhuo.com///web-crawler-crawl4j/</link>
        <guid isPermaLink="true">http://jasonzhuo.com///web-crawler-crawl4j/</guid>
      </item>
    
      <item>
        <title>Random forest clustering</title>
        <description>&lt;h3&gt;随机森林聚类（Random forest clustering）&lt;/h3&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2015-05-23-randforest.png&quot; alt=&quot;image&quot; /&gt;
随机森林聚类分析&lt;/p&gt;

&lt;!-- more --&gt;


&lt;h3&gt;1.起因&lt;/h3&gt;

&lt;p&gt;网络上现存的很多资料都是关于随机森林的分类和回归分析，但很少有材料讲解随机森林的聚类分析过程。
随机森林以及随机森林的分类回归过程我就不做详细介绍了，你可以参考网络上的其他资料 [1]，[2], [3], [8]。&lt;/p&gt;

&lt;p&gt;分析随机森林聚类算法还有如下的原因：&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;GBDT和随机森林在ESL书里面说是目前最优的两种算法&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;然后呢，最近也在研究一些新的聚类算法，就顺带研究一些随机森林是如何用了聚类的吧。&lt;/p&gt;

&lt;h3&gt;2.随机森林聚类&lt;/h3&gt;

&lt;p&gt;随机森林聚类算法的最核心思想：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. 将非监督学习转换为监督学习&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. 相似度矩阵的计算&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;合成数据集（synthetic dataset）中包括的样本是通过样本合成算法产生新的样本的集合。最常用的就是合成算法就是生成和原始数据集（Orignial dataset）样本数量一致，相同特征的集合。&lt;/p&gt;

&lt;p&gt;举个网上的例子[7]来说吧：
假设有两个特征\(x_1\),\(x_2\).其中，\(x_1\)是连续变量服从\(Normal(0,1)\),\(x_2\)是0，1分布。一种合成数据的方法是根据每个特征的分布进行估计，值得注意的是随机生成的数据是各自独立的。另外一种合成方法是根据样本的概率进行随机选择，例如在样本中\(P(male)=0.4\),\(P(female)=0.6\)，那么合成的\(x_1\)的值就从原始数据集里面随机选择一个，合成的\(x_2\)的值就按照上述概率生成0或1。&lt;/p&gt;

&lt;p&gt;新的训练样本集合多了一个新的特征，该特征用来区分该样本是合成的还是原始的，用二进制表示即可。&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;新的训练样本集合（多一个类标签特征）=合成样本数据集合+原始数据集合&lt;/p&gt;&lt;/blockquote&gt;

&lt;h4&gt;2.1相似度矩阵&lt;/h4&gt;

&lt;p&gt;随机森林在建模的同时，还提供了样本相似性度量，即相似度矩阵(简记为 Prox 矩阵)。当用一棵树对所有数据进行判别时，这些数据最终都将达到该树的某个叶节点上.可以用两个样本在每棵树的同一个节点上出现的频率大小，来衡量这两个样本之间的相似程度，或两个样本属于同一类的概率大小。&lt;/p&gt;

&lt;p&gt;Prox矩阵\(P=p_{ij} \)生成过程如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;对于样本数为N的训练集合，首先生成一个\(N\times N\)的矩阵，\(p_{ii}=1\)，其他元素为0。&lt;/li&gt;
&lt;li&gt;对于任意两个样本\(x_i\),\(x_j\),若他们出现在生成的同一棵树上的同一个叶节点上，则$$p[i][j]= 1+p[i][j]$$&lt;/li&gt;
&lt;li&gt;重复上述过程直到m棵树全部生成，得到相应的矩阵&lt;/li&gt;
&lt;li&gt;进行归一化处理$$p[i][j]= \frac{p[i][j]}{N}$$&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;在随机森林进行聚类的过程中，运用到到了两个样本之间的相似度进行计算。我们知道在聚类算法中存在很多衡量两个样本之间距离的方法，欧几里得距离等等。但是在随机森林中的距离是如下计算的[5]:&lt;/p&gt;

&lt;p&gt;$$DISSIM[i][j]= \sqrt{1-p[i][j]}$$&lt;/p&gt;

&lt;h4&gt;2.2随机森林聚类计算过程&lt;/h4&gt;

&lt;p&gt;随机森林聚类算法：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;生成合成样本集合&lt;/li&gt;
&lt;li&gt;生成新的带类新标签的训练集合N，样本只包括两类：合成的数据，原始的数据&lt;/li&gt;
&lt;li&gt;根据N生成相似度矩阵P&lt;/li&gt;
&lt;li&gt;将P中被我们标记为合成数据的行和列删除&lt;/li&gt;
&lt;li&gt;运用其他聚类算法进行聚类&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;为啥我们要生产合成数据集，然后还要将其加入原始训练集合呢？我直观地认为直接在原始数据集上算相似性不就ok啦。后来我反应过来，但是原始数据是不带类标签的（非监督学习）。产生合成数据集的目的就是和原始数据进行类标记，将非监督学习转换为监督学习。&lt;/p&gt;

&lt;p&gt;步骤5中居然采用还是其他的聚类算法，只不过替换了距离测量方法，这点感觉有点那个啥&lt;img src=&quot;/assets/smilies/30.gif&quot; id=&quot;similey&quot;&gt;。&lt;/p&gt;

&lt;h4&gt;2.3随机森林聚类应用&lt;/h4&gt;

&lt;p&gt;文献[3]和文献[4]主要是用随机森林对医疗疾病属性的聚类，文献[3][4]用的是PAM（Partitioning around medoids）聚类算法。
随机森林的Prox矩阵可以作为多个基于不相似度聚类算法的输入，例如文献[5]中同样利用的是K-Medoids聚类算法PAM。作者将其应用到网络流量的聚类上，还比较创新。&lt;/p&gt;

&lt;h4&gt;2.4随机森林聚类Case study&lt;/h4&gt;

&lt;p&gt;利用R语言中的函数&lt;code&gt;rfClustering(model,noClusters=4)&lt;/code&gt;进行聚类分析，这个例子中没有体现出合成数据集合的生成的，因为iris数据集是带类标签的。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;set&amp;lt;-iris&lt;/code&gt; 载入iris数据集，iris数据集是带类标签的&lt;/li&gt;
&lt;li&gt;&lt;code&gt;md&amp;lt;-CoreModel(Species ~., set,model=&quot;rf&quot;,rfNoTrees=30)&lt;/code&gt; 利用随机森林得到模型，模型md中包括了相似度矩阵&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mdCluster&amp;lt;-rfClustering(md,3)&lt;/code&gt;聚成3类&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;code&gt;rfClustering&lt;/code&gt;这个函数根据作者介绍，内在是调用的PAM聚类算法的。&lt;/p&gt;

&lt;p&gt;聚类结果：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2015-05-23rfresult.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;参考文献&lt;/h3&gt;

&lt;p&gt;[1]&lt;a href=&quot;http://www.cnblogs.com/leftnoteasy/archive/2011/03/07/random-forest-and-gbdt.html&quot;&gt;http://www.cnblogs.com/leftnoteasy/archive/2011/03/07/random-forest-and-gbdt.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2]&lt;a href=&quot;http://blog.sciencenet.cn/blog-661364-615921.html&quot;&gt;http://blog.sciencenet.cn/blog-661364-615921.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3]&lt;a href=&quot;http://blog.csdn.net/songzitea/article/details/10035757&quot;&gt;http://blog.csdn.net/songzitea/article/details/10035757&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4]Shi T, Seligson D, Belldegrun A S, et al. Tumor classification by tissue microarray profiling: random forest clustering applied to renal cell carcinoma[J]. Modern Pathology, 2005, 18(4): 547-557.&lt;/p&gt;

&lt;p&gt;[5]Shi T, Horvath S. Unsupervised learning with random forest predictors[J]. Journal of Computational and Graphical Statistics, 2006, 15(1).&lt;/p&gt;

&lt;p&gt;[6]Wang Y, Xiang Y, Zhang J. Network traffic clustering using Random Forest proximities[C]//Communications (ICC), 2013 IEEE International Conference on. IEEE, 2013: 2058-2062.&lt;/p&gt;

&lt;p&gt;[7]&lt;a href=&quot;http://stats.stackexchange.com/questions/92725/unsupervised-random-forest-using-weka&quot;&gt;http://stats.stackexchange.com/questions/92725/unsupervised-random-forest-using-weka&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[8]&lt;a href=&quot;http://www.zilhua.com/629.html&quot;&gt;http://www.zilhua.com/629.html&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 23 May 2015 00:00:00 +0800</pubDate>
        <link>http://jasonzhuo.com///random-forest-clustering/</link>
        <guid isPermaLink="true">http://jasonzhuo.com///random-forest-clustering/</guid>
      </item>
    
      <item>
        <title>Software Define Network</title>
        <description>&lt;h3&gt;软件定义网络初探&lt;/h3&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2015-05-14SDN1.png&quot; alt=&quot;image&quot; /&gt;
软件定义网络Software Define Network(SDN:Still don&#39;t know)初探笔记，以及Mininet原理分析&lt;/p&gt;

&lt;!-- more --&gt;


&lt;h3&gt;软件定义网络的定义&lt;/h3&gt;

&lt;p&gt;如果将网络中的所有网络设备视为被管理的资源，那么就像操作系统一样，软件定义网络SDN提供了同样的管理视图和编程接口。这样基于SDN这个平台，用户可以开发各种应用程序，通过软件来定义逻辑上的网络拓扑，以满足对网络资源的不同需求，二维无需关系底层网络的拓扑结构。&lt;/p&gt;

&lt;h3&gt;SDN交换机与控制器&lt;/h3&gt;

&lt;p&gt;SDN交换机只负责网络高速转发，保存的用于转发决策的转发表信息来自控制器，&lt;strong&gt;SDN交换机需要在远程控制器的管控下工作&lt;/strong&gt;，与之相关的设备状态和控制指令都需要经由SDN的南向接口传达，从而实现集中化统一管理。控制指令标准和状态需要满足SDN协议规范。
下图说明了SDN与交换机的工作流程&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2015-05-15sdn2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;控制器目前有很多，根据知乎网友的推荐[link][http://www.zhihu.com/question/22599089], 控制器如果选择Python 的话，推荐 POX 或者 Ryu，更推荐 Ryu 一些，Java 的话就是 Floodlight 了。&lt;/p&gt;

&lt;h3&gt;术语区分&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Mininet&lt;/strong&gt;: 主要是虚拟出Openflow交换机以及host节点，并且可以自定义拓扑结构，是一个网络仿真平台。支持Openflow，OpenvSwitch等软件定义网络布局。
&lt;strong&gt;Open vSwitch（OVS）&lt;/strong&gt;:是由 Nicira Networks 主导的，运行在虚拟化平台（例如 KVM，Xen）上的虚拟交换机。
&lt;strong&gt;Opendaylight（ODL）&lt;/strong&gt;: 属于控制器的一种，开源项目，整体SDN解决方案，包含一系列组件，支持多种协议包括了openflow。是现在主流的控制器项目，功能比较完善
&lt;strong&gt;floodlight&lt;/strong&gt;: 属于控制器的一种，开源项目，文档齐全，图形化界面管理。简而言之，Floodlight提供了用户友好的图形化界面，来控制管理支持Openflow协议的交换机。
&lt;strong&gt;Openflow&lt;/strong&gt;: 是用于管理交换机流表的协议，起源于斯坦福大学的Clean Slate项目组，属于SDN协议的一种（其他的例如ForCES、PCE-P等等）。Openflow应用最广，现已成为了SDN的代言词。OpenFlow协议是描述控制器和交换机之间交互信息的南向接口标准。控制器和交换机之间通过这SDN协议进行连接建立，流表下发和信息交换，实现对网络中所有OpenFlow交换机的控制。&lt;/p&gt;

&lt;h3&gt;私有云与SDN的关系&lt;/h3&gt;

&lt;p&gt;私有云(Private Clouds)是为特定客户单独使用而构建的一个资源服务网络。特定用户拥有基础设施，并可以控制在此基础设施上部署需求的应用程序和服务。私有云的核心属性是专有资源。&lt;/p&gt;

&lt;p&gt;网络是私有云中的瓶颈问题。现在，服务器和存储技术已经发展成共享资源，云管理员可以自由地调用这些资源，但是网络却仍然是手动的。为了提高灵活性，私有云网络必须进行虚拟化，而软件定义网络（SDN）是一个性价比不错的方法。&lt;/p&gt;

&lt;p&gt;根据&lt;a href=&quot;https://www.opennetworking.org/solution-brief-how-openflow-based-sdn-transform-private-cloud&quot;&gt;ONF&lt;/a&gt;介绍，SDN是私有云的基础。SDN使得私有云可以分享设备资源，按需分配，自动操作，处理动态变化的事务更加灵活有效，最大化资源利用。&lt;/p&gt;

&lt;h3&gt;Mininet运行原理&lt;/h3&gt;

&lt;p&gt;Mininet通过Linux内置的Network Namespace来达到主机之间通信的隔离效果。这个Namespace和C++中的Namespace感觉差不多，不过C++Namespace达到的效果是函数的隔离。我觉得这篇blog介绍Linux Network namespace比较清楚，&lt;a href=&quot;http://neokentblog.blogspot.hk/2014/07/linux-network-namespace.html&quot;&gt;Network Namespace 介绍&lt;/a&gt;,通过这个介绍，Mininet理解起来就方便多了。&lt;/p&gt;

&lt;p&gt;在Mininet中我们不单可以在主机上面运行ping命令，每一条 Linux下的命令或者程序都可以在 Mininet 中的虚拟主机中运行。但是每个虚拟主机 h1，h2，s1三个进程列表是完全相同的。其实完全可以做到各个主机完全独立，就想 LXC 那样，但是目前 Mininet 并没有这么做。在 Mininet 中所有的进程都放在 root 下面，这样你可以在 Linux的 shell 中直接用kill或者ps这些命令查看或者杀死进程。&lt;a href=&quot;http://segmentfault.com/a/1190000000669218&quot;&gt;参考外链&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ww4.sinaimg.cn/large/65c83a2bjw1dwmvastgo4j.jpg&quot; alt=&quot;Mininet原理&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;其它&lt;/h3&gt;

&lt;p&gt;通过mininet可以在本机迅速搭建任意拓扑结构的虚拟网络。然后利用控制器可以控制和管理所构建的虚拟网络，增加设备，删除设备等等。&lt;/p&gt;

&lt;p&gt;最简单的配置就是只有一台主机即可，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2015-05-15sdn3.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;当然复杂一点可以利用多台主机，比如这样：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2015-05-15SDN4.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;最近实验室采购设备，调研至此感觉SDN好像不需要特殊设备呢？我们只需要在不同host上运行不同的程序，甚至不同的操作系统，以方便网络安全实验。不过现在有很多厂家在做SDN控制器，还有支持Openflow协议的交换机，但都还不成熟。目前业界尚未发布完全符合 OpenFlow 协议规范的芯片，所以说感觉没必要买一些不成熟的产品。就多买几台性能还行的服务器，交换机，其余硬件资源没必要，在剩下的就是手动搭建工作了。&lt;/p&gt;
</description>
        <pubDate>Thu, 14 May 2015 00:00:00 +0800</pubDate>
        <link>http://jasonzhuo.com///sdn/</link>
        <guid isPermaLink="true">http://jasonzhuo.com///sdn/</guid>
      </item>
    
      <item>
        <title>Encrypted Traffic identification</title>
        <description>&lt;p&gt;&lt;img src=&quot;/assets/images/2015-05-11enflow2.png&quot; alt=&quot;image&quot; /&gt;
2015加密流量检测论文阅读笔记，持续更新（last update 2015.9.29）&lt;/p&gt;

&lt;!-- more --&gt;


&lt;h4&gt;目前还有效的加密流量识别方法&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;基于端口号的，局限性很大但某些场景仍然有效。&lt;/li&gt;
&lt;li&gt;基于内容签名的。有些加密协议有固定特殊的内容特征。&lt;/li&gt;
&lt;li&gt;基于流特征的。加密协议在进行密钥协商时具有特殊过程。&lt;/li&gt;
&lt;li&gt;基于主机行为的。从主机行为来看加密协议的建立过程，局限性仍然很大。&lt;/li&gt;
&lt;/ol&gt;


&lt;hr /&gt;

&lt;h4&gt;论文[1]作者的方法&lt;/h4&gt;

&lt;p&gt;上述方法的局限性较大。论文[1]中作者采用了随机度测试的方法来对加密流量进行识别。首先作者利用了\(l_{1}-norm\) regularized 逻辑斯特回归来选择&lt;strong&gt;sparse&lt;/strong&gt;特征（特征中有很多0值），然后利用了ELM来对加密流量进行识别，选择ELM的原因是其具有更好的识别效果和更快的识别速度。&lt;/p&gt;

&lt;p&gt;作者在论文[1]中利用随机度测试方法获取了188维度的特征，然后利用一范数正则化方法对特征进行降维。最后再利用ELM学习算法，对加密流量进行识别。&lt;/p&gt;

&lt;p&gt;识别效果和支持向量数据描述(support vector data description)SVDD以及GMM方法进行了对比。可见作者采用了one-class分类。最后的结果是识别率大概在80%左右。&lt;/p&gt;

&lt;h4&gt;随机度测试与加密方法&lt;/h4&gt;

&lt;p&gt;如何衡量加密算法的强弱好坏呢。有一种方法就是加密密文需要通过一定的随机度测试，输出的密文需要是随机的，或者近似随机的，这样的加密效果才好。衡量随机度的参数P-value越大则随机的可能性越高。参考：&lt;a href=&quot;http://www.zhihu.com/question/20222653&quot;&gt;知乎：如何评价一个伪随机数生成算法的优劣&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;下图比较形象的说明了随机数测试的问题&lt;a href=&quot;https://www.random.org/analysis/&quot;&gt;cited from https://www.random.org/analysis/ &lt;/a&gt;
&lt;img src=&quot;https://www.random.org/analysis/dilbert.jpg&quot; alt=&quot;Cited &quot; /&gt;
现在随机度测试的方法很多，如：NIST test set, DiEHARD test set等等，其中以美国国家标准与技术研究所的NIST最为著名：&lt;a href=&quot;http://csrc.nist.gov/groups/ST/toolkit/rng/documentation_software.html&quot;&gt;NIST传送门&lt;/a&gt;，&lt;a href=&quot;http://blog.csdn.net/Tom_VS_Jerry/article/details/26086099&quot;&gt;NIST安装过程介绍&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;NIST SP800-22 测试标准包含15个测试项，每个测试项都是针对被测序列的某一特性进行检测的，如下图所示。图片引用自[2]&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2015-05-11enflow.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;NIST15种随机性测试简介[8]&lt;/h3&gt;

&lt;h4&gt;1.频率测试(Monobit Test)&lt;/h4&gt;

&lt;p&gt;检验目的： 测试0和1在整个序列中所占的比例。如果0和1在整个序列中出现的频率都接近0.5，那么就是说在整个序列中0和1出现的概率基本一致，因此序列是随机的。&lt;/p&gt;

&lt;p&gt;检验过程：&lt;/p&gt;

&lt;p&gt;1.将序列中的0映射为“+1”，0映射为“-1”，然后对整个序列求和得到\(S_n\)&lt;/p&gt;

&lt;p&gt;2.计算统计检验量\(S_{obs}=\frac{|S_n|}{\sqrt{n}}\),其中\(n\)
为序列长度&lt;/p&gt;

&lt;p&gt;3.计算P-value:&lt;/p&gt;

&lt;p&gt;$$P=erfc(\frac{S_{obs}}{\sqrt{2}})$$&lt;/p&gt;

&lt;p&gt;其中erfc(Complementary error function)定义为:&lt;/p&gt;

&lt;p&gt;$$erfc(z)=\frac{2}{\sqrt{\pi}} \int_{z}^{\infty} e^{-u^{2}} du$$&lt;/p&gt;

&lt;p&gt;数学原理：当测试次数充分多时，二项分布B(n,p)逼近正态分布N(np,npq)，\(S_n\)又叫做规范和，它被正则化后就服从正态分布了。
$$Z_n=\frac{S_n-E(S_n)}{\sqrt{D(S_n)}} \rightarrow N(0,1)$$
做假设检验：
$$P-value = 2[1-\phi(|S_{obs}|)]=erfc(\frac{s(obs)}{\sqrt{n}})$$
如果Pvalue&gt;0.01那么就说该序列为随机序列，反之则不是。如果Pvalue的值太小，那么就表明Sobs或者\(S_n\)较大引起的，表明序列中要么0特别多，要么1特别多，这样就不是随机的。&lt;/p&gt;

&lt;h4&gt;2.分块频数测试&lt;/h4&gt;

&lt;p&gt;检验目的：主要是看M位的字块中“1”出现的次数，如果比例接近为M/2那么就是随机的。当M=1时，变化为频数测试。&lt;/p&gt;

&lt;p&gt;测试过程：
1. 将序列分为长度为M的N块，因此n=MN
2. 由于这N个相互独立的随机序列均服从标准正态分布，这N个服从标准正态分布随机变量的平方和，构成新的随机变量，服从卡方分布，其自由度为N
3. 利用卡方分布的假设检验过程，判断原序列是否是随机的。&lt;/p&gt;

&lt;p&gt;该测试也是看Pvalue的值，如果Pvalue&gt;0.01那么就说该序列为随机序列，反之则不是。&lt;/p&gt;

&lt;h4&gt;3.游程测试&lt;/h4&gt;

&lt;p&gt;检验目的： 游程是指一个没有间断的相同数序列。长度为k的游程包括k个相同单位。游程测试的目的是判断不同长度的“1”游程数目，以及“0”游程的数目是否跟理想的随机序列期望一致。具体来讲，就是看这样的“0”“1”子块之间震荡是否太快或者太慢。&lt;/p&gt;

&lt;p&gt;测试过程：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;计算检验前输入序列中“1”的出现频率&lt;/li&gt;
&lt;li&gt;判断游程测试的前提条件是否满足，如果不满足则退出测试。&lt;/li&gt;
&lt;li&gt;计算检验统计量Vn(obs), 其中\(V_{obs}\),0到1或者1到0的变化次数和。&lt;/li&gt;
&lt;li&gt;计算Pvalue的值&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;数学原理：当测试次数充分多时，二项分布B(n,p)逼近正态分布N(np,npq)，\(V_{obs}\)又叫做规范和，它被正则化后就服从正态分布了,如果Vn(obs)的值很大，则表明震荡很快，反之表明震荡很慢。&lt;/p&gt;

&lt;h4&gt;4.块内最长游程检验&lt;/h4&gt;

&lt;p&gt;检验目的： 该检验主要是看长度为M-bits的字块长度中最长的“1”游程。检验目的是判断待检序列中的最长“1”游程长度是否同随机序列一致。&lt;/p&gt;

&lt;p&gt;测试过程：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;将序列分作长度为M-bit子块，程序里面规定了3种不同的字块长度，分别为8，128，10000，n= M*N&lt;/li&gt;
&lt;li&gt;将各个字块中的最长“1”游程频率列表&lt;/li&gt;
&lt;li&gt;计算卡方分布的P值&lt;/li&gt;
&lt;/ol&gt;


&lt;h4&gt;5.二元矩阵秩检验&lt;/h4&gt;

&lt;p&gt;检验目的： 该检验主要是看整个序列的分离子矩阵的秩。目的是核对源序列中固定长度子链间的线性依赖关系。&lt;/p&gt;

&lt;h4&gt;6.离散傅里叶变换检验&lt;/h4&gt;

&lt;p&gt;检验目的：本检验主要是看对序列进行分步傅里叶变换后的峰值高度。目的是探测待检验信号的周期性,以此揭示其与相应的随机信号之间的偏差程度。做法是观察超过 95%阈值的峰值数目与低于5%峰值的数目是否有显著不同。&lt;/p&gt;

&lt;h4&gt;未完待续。。。&lt;/h4&gt;

&lt;h4&gt;特征提取与选择&lt;/h4&gt;

&lt;p&gt;作者在文中利用NIST测试套件，通过不同的参数调节产生不同的测试小项，总共提取了188维度的特征。然后作者利用稀疏特征提取(Sparse Feature selection)。并在逻辑斯特回归中加入了1范数罚参。&lt;/p&gt;

&lt;h4&gt;ELM 简介&lt;/h4&gt;

&lt;p&gt;传统的SVM和神经网络需要人们进行干预，学习速度较慢，学习泛化能力也比较差。极限学习机具有较快的学习速度以及良好的泛化性能。然而，它的性能还可以得到很大提高，主要基于两个原因[3]：(1)极限学习机网络中的隐层节点可以减少；(2)网络参数不必每次都调节。&lt;/p&gt;

&lt;p&gt;我感觉ELM和传统神经网络学习过程中最大的区别就在于如何求解最小化损失函数。传统的神经网络在学习过程中，学习算法需要在迭代过程中调整所有参赛。而在ELM算法中，直接通过求H的广义逆矩阵，输出权重\(\beta\)就可以被确定。&lt;/p&gt;

&lt;p&gt;为啥不需要调节参数了呢，原来Huang 已经证明了具有随机制定的输入权值、隐层阈值和非零激活函数的单隐层前馈神经网络可以&lt;strong&gt;普遍近似&lt;/strong&gt;任何具有紧凑输入集的连续函数。由此可以看出，输入权值和隐层阈值可以不必调节[3]。&lt;/p&gt;

&lt;p&gt;另外可以参考&lt;a href=&quot;http://blog.csdn.net/google19890102/article/details/18222103&quot;&gt;ELM算法简介&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;以及&lt;a href=&quot;http://www.ntu.edu.sg/home/egbhuang/&quot;&gt;ELM intro&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;还有&lt;a href=&quot;http://blog.csdn.net/itplus/article/details/9277721&quot;&gt;ELM算法基础&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;论文[6]阅读笔记&lt;/h4&gt;

&lt;p&gt;作者在文章中介绍了加密流量识别的基本情况，强调了最近的进展和未来的发展可能会遇到的挑战。文章中指出，加密流量类型主要集中在：SSH，VPN，SSL,加密P2P，加密VoIP，匿名网络流量。&lt;/p&gt;

&lt;p&gt;仅仅粗粒度区分加密和非加密流量是远远不够的，现实世界迫切需要的是从加密中获取应用层类型。这显然是相当复杂的工作，许多方法的综合运用才可能达到该目的。另外，协议混淆（traffic obfuscation）和流量变形（traffic morph）可以达到绕过基于机器学习的统计分类的效果，同样是未来的挑战之一。论文没有开放下载，只读了其中能看到的部分，就已经读到的内容来看，感觉文章综述很一般。&lt;/p&gt;

&lt;h3&gt;SSL/TLS应用层流量识别&lt;/h3&gt;

&lt;h4&gt;1.论文[4]阅读笔记&lt;/h4&gt;

&lt;p&gt;在未加密情况下识别应用层类型是比较成熟的，然而在加密情况下识别就相对困难许多。&lt;/p&gt;

&lt;p&gt;作者在文章中提出了使用随机指纹（&lt;em&gt;stochastic fingerprints&lt;/em&gt;）来识别SSL/TLS会话过程中的应用类型。指纹是基于一阶马尔可夫链的，然后马尔可夫链的参数是通过训练集合学习得到的。由于参数的不同性，该方法能有效的检测出应用层类型，以及异常的SSL会话过程。&lt;/p&gt;

&lt;p&gt;该方法和一些其他的SSL指纹识别方法比较类似，也是利用SSL/TLS握手过程中的头部信息。不同之处在于作者利用的是基于马尔可夫链随机指纹，将不同应用层类型的SSL/TLS握手过程的状态转移形成带有不同参数的马尔科夫状态转移链。作者在论文中对12项常用的SSL类型进行了建模：例如PayPal,Twitter,Skype等等。&lt;/p&gt;

&lt;h4&gt;2.论文[5]阅读笔记&lt;/h4&gt;

&lt;p&gt;传统的基于载荷的和机器学习的方法给系统造成的负载较大。作者提出了利用基于指纹和统计分析的混合方法来解决该问题。作者首先使用指纹来检测出SSL/TLS，然后再利用统计分析来找出确定的应用层类型。实验结果表明，该方法能够99%识别SSL/TLS流量，并达到F-score为94.52%的应用层识别效果。&lt;/p&gt;

&lt;p&gt;作者识别SSL/TLS流量的方法主要还是基于流量关键字段特征，这点和普通方法没区别。作者将SSL/TLS流量识别出来之后，又用了贝叶斯方法来对加密流的特征进行学习，统计特征包括：平均包长度，最大最小包长度，平均包到达间隔时间，流持续时间，流所包括的包数量。总体感觉创新不是很大。&lt;/p&gt;

&lt;h4&gt;3.论文[7]阅读笔记&lt;/h4&gt;

&lt;p&gt;论文作者根据加密会话数据内容中常见字符和非常见字符出现概率相近，非加密会话常见字符出现次数多，而非常见字符出现次数少的现象，设计了一个根据信息熵的加密会话识别方法。作者认为，不同加密会话信息熵的统计结果服从正态分布，加密会话的信息熵取值范围由均值\(\mu\)和方差\(\sigma\)决定。作者提出了字符信息熵值H，只要在\(H\in [\mu-2\sigma, \mu+2\sigma] \)，则认为是加密会话。字符信息熵值和普通信息熵值在计算形式上只有一个分母上的差别，和我原来看过的一篇论文比较类似。另外，作者并没有说明为什么取2倍的置信域而不是3倍置信域，均值和方差的设置是经过大量实验取的经验数据，而没有较为科学的取值依据。论文并未有很好地说明为什么在拥塞网络环境下，识别准确率会降低的原因。&lt;/p&gt;

&lt;h4&gt;参考文献&lt;/h4&gt;

&lt;p&gt;[1]Meng J, Yang L, Zhou Y, et al. Encrypted Traffic Identification Based on Sparse Logistical Regression and Extreme Learning Machine[M]//Proceedings of ELM-2014 Volume 2. Springer International Publishing, 2015: 61-70.&lt;/p&gt;

&lt;p&gt;[2]侯佳音, 萧宝瑾. 随机数测试标准与随机数发生器性能的关系[J]. 2012.&lt;/p&gt;

&lt;p&gt;[3]王建功. 基于极限学习机的多网络学习[J]. 2010.&lt;/p&gt;

&lt;p&gt;[4]Korczynski M, Duda A. Markov chain fingerprinting to classify encrypted traffic[C]//INFOCOM, 2014 Proceedings IEEE. IEEE, 2014: 781-789.&lt;/p&gt;

&lt;p&gt;[5]Sun G L, Xue Y, Dong Y, et al. An novel hybrid method for effectively classifying encrypted traffic[C]//Global Telecommunications Conference (GLOBECOM 2010), 2010 IEEE. IEEE, 2010: 1-5.&lt;/p&gt;

&lt;p&gt;[6]Cao Z, Xiong G, Zhao Y, et al. A Survey on Encrypted Traffic Classification[M]//Applications and Techniques in Information Security. Springer Berlin Heidelberg, 2014: 73-81.&lt;/p&gt;

&lt;p&gt;[7]陈利,张利,班晓芳等.基于信息熵的加密会话检测方法[J].计算机科学,2015,42(1):142-143,174.DOI:10.11896/j.issn.1002-137X.2015.1.033.&lt;/p&gt;

&lt;p&gt;[8]Rukhin A, Soto J, Nechvatal J, et al. A statistical test suite for random and pseudorandom number generators for cryptographic applications[R]. Booz-Allen and Hamilton Inc Mclean Va, 2001.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Written with &lt;a href=&quot;https://stackedit.io/&quot;&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;&lt;/blockquote&gt;
</description>
        <pubDate>Mon, 11 May 2015 00:00:00 +0800</pubDate>
        <link>http://jasonzhuo.com///encrypted-traffic-identification/</link>
        <guid isPermaLink="true">http://jasonzhuo.com///encrypted-traffic-identification/</guid>
      </item>
    
      <item>
        <title>Botnet detection</title>
        <description>&lt;p&gt;&lt;img src=&quot;/assets/images/2015-05-08botnet.png&quot; alt=&quot;image&quot; /&gt;
Botnet检测论文阅读笔记,last update (2015.8.11)&lt;/p&gt;

&lt;!-- more --&gt;


&lt;h4&gt;僵尸网络检测方法面临的问题&lt;/h4&gt;

&lt;p&gt;和流量识别所面临的问题类似[1]，僵尸网络的检测同样也缺乏公开可用的测试数据集合通用的测试评估方法。在论文[2]中，作者提出了僵尸网络检测面临的问题：原有僵尸网络检测论文都只是提出了一种新方法，但是都没有做横向对比。其原因主要是因为分享包含僵尸网络流量数据比较困难，缺乏好的数据集，缺乏科学有效的对比方法等。&lt;/p&gt;

&lt;p&gt;作者在论文[2]中首次提出了他们的解决办法，并对3种常见的僵尸网络检测手段在自己收集整理的数据集合上做了横向对比。作者还将数据集合公开出来，极大的方便了其他研究人员的继续研究------&lt;a href=&quot;http://mcfp.weebly.com/the-ctu-13-dataset-a-labeled-dataset-with-botnet-normal-and-background-traffic.html&quot;&gt;数据集传送门&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;原有的论文都是自己提出了一种方法后，自己搜集了一些数据集进行测试。这些数据集往往很难获取，数据集合的真实性也难以保证和真实网络环境类似。同时，自己构造的数据集很难和其他方法进行对比。（不同的方法针对的数据集特征可能会大相径庭）除了数据集的问题之外，很多僵尸检测的论文中的评价指标仅局限于FPR，或者使用的是不同的评价定义方式，这给方法之间的评比造成困难。&lt;/p&gt;

&lt;h4&gt;1. CAMNEP&lt;/h4&gt;

&lt;p&gt;CAMNEP(Cooperative Adaptive Mechanism for NEtwork Protection) 是一个基于异常检测的网络行为的分析系统。系统包括了state of art的异常检测方法。
主要包括3部分：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;异常检测器： 通过多种异常检测模块，对网络行为进行分析&lt;/li&gt;
&lt;li&gt;信任模型：上一个输出的结果会和信任模型进行对比。信任模型会将Netflow根据其异常值和代表的事件类型聚集为不同类别。并持续对异常值进行更新来达到减少误报率。&lt;/li&gt;
&lt;li&gt;异常聚集器：结果汇总，对每个异常检测器的判定结果加权平均等。&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;CAMNEP利用NetFlow分析网络异常流量也存在一些限制条件，如需要网络设备对NetFlow的支持，需要分析NetFlow数据的工具软件，需要网络管理员准确区分正常流量数据和异常流量数据等。&lt;/p&gt;

&lt;h5&gt;1.1异常检测模块&lt;/h5&gt;

&lt;p&gt;下面介绍的异常检测模块是经过对原来算法的改进或者修改实现的，可能会和原来论文中的方法有差异。
&lt;strong&gt;MINDS&lt;/strong&gt;[3]，一个入侵检测模块，提取的特征包括：1.相同源IP地址发出的Netflow数，2.到达相同Host的Netflow数，3.从相同端口号到达相同Host的Netflow数，4.从相同Host到达相同端口号的Netflow数. 某种情况下（Context）下的异常值，为上述四个特征到正常样本的距离。最后的Global distance是各个Context下异常值的sum of their squares.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Xu et al. 2005&lt;/strong&gt; 提出的方法是将来自同一个源IP地址的Netflow进行特征提取。特征包括：1. 正则化后的源端口熵值（Normalized entropy of the source ports），2. 正则化后的目的IP地址熵值，3. 正则化后的目的端口熵值。某种情况下（Context）下的异常值，为上述3个特征到正常样本的距离。最后的distance同样是各个Context下异常值的sum of their squares.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lakhina volume&lt;/strong&gt; 原文章是来自Sigcomm的文章，针对的是Network-wide的异常检测。在[1]中作者修改了其中一部分，其主要思想是将来自同一个源IP地址的数据流用PCA方法分为正常流和非正常流，因为正常流占了绝大多数成分而异常流只存在少部分。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lakhina entropy&lt;/strong&gt; 基于上述PCA的思想，但是采用了和xu et al.类似的特征。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TAPS 2006&lt;/strong&gt; 与上述方法都不相同。该方法针对的是横向和纵向端口扫描。该方法的基本思想还是通过提取特征，然后设置特征阈值。超过一定的阈值则判断为扫描行为。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;KGB 2012&lt;/strong&gt; 基于的是  Lakhina 的工作，也是利用的PCA来分解每个源IP产生的特征向量。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Flag&lt;/strong&gt; 和KGB方法类似，只不过输入的特征向量不同。Flag是基于来自同一个IP地址的TCP标志柱状图 。该检测器就是寻找一个连续的TCP标志异常组合。&lt;/p&gt;

&lt;h5&gt;1.2可信模型建立&lt;/h5&gt;

&lt;p&gt;CAMNEP的可信模型类似于Kmeans中的聚类。模型计算每个Netflow到每个可信簇类中心(Centroid)的距离。&lt;/p&gt;

&lt;h4&gt;2. BGlus&lt;/h4&gt;

&lt;p&gt;BGlus是基于行为的僵尸网络检测方法。其基本思想是先对已知的僵尸网络流量进行模型抽象，然后在网络上寻找相似的流量。
其基本步骤如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;将Netflows按时间窗口进行分割&lt;/li&gt;
&lt;li&gt;将分割后的Netflows按照相同的源IP进行组合&lt;/li&gt;
&lt;li&gt;对不同的源IP流量进行聚类&lt;/li&gt;
&lt;li&gt;该步只针对训练阶段：对僵尸网络聚类进行ground truth 打标签&lt;/li&gt;
&lt;li&gt;该步只针对训练阶段：利用僵尸网络簇训练一个分类器。&lt;/li&gt;
&lt;li&gt;该步只针对测试阶段：利用分类器来检测聚类结果中的僵尸网络簇&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;关于作者在第二步中为什么这么做。作者假设这样做会产生新的patterns，可以帮助我们识别僵尸网络，这样做偏向于基于主机的行为分析。&lt;/p&gt;

&lt;p&gt;作者在BGlus中运用的是EM聚类算法（作者假设不同的流量产生于相同的分布）和JRIP分类算法。&lt;/p&gt;

&lt;h4&gt;3. BotHunter&lt;/h4&gt;

&lt;p&gt;BotHunter是基于状态序列的匹配方法。它有一个关联分析引擎来分析恶意软件当前的状态过程。检测的特征过程包括：inbound scanning, exploit usage, egg download, outbound bot coordination dialog, outbound attack propagation.&lt;/p&gt;

&lt;p&gt;BotHunter是经过修改过的Snort软件。主要增加了两个插件，一个是SCADE(statistical scan anomaly detection engine) ,另外一个是SLADE(statistical payload anomaly detection engine)&lt;/p&gt;

&lt;p&gt;下面穿插对比一下几个检测器：
&lt;strong&gt;Bothunter&lt;/strong&gt;: Vertical Correlation. Correlation on the behaviors of single host.
&lt;strong&gt;Botsniffer&lt;/strong&gt;: Horizontal Correlation. On centralized C&amp;amp;C botnets
&lt;strong&gt;Botminer&lt;/strong&gt;: Extension on Botsniffer, no limitations on the C&amp;amp;C types.&lt;/p&gt;

&lt;h4&gt;4. 数据集构建&lt;/h4&gt;

&lt;p&gt;论文中作者构建的数据集具有以下特点：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;是真实的僵尸网络程序，而不是仿真&lt;/li&gt;
&lt;li&gt;数据集中包括未知流量&lt;/li&gt;
&lt;li&gt;数据集中存在类标，方便训练和方法效果评估&lt;/li&gt;
&lt;li&gt;包括多种僵尸网络流量&lt;/li&gt;
&lt;li&gt;包括多个僵尸主机同时被感染的情况&lt;/li&gt;
&lt;li&gt;包括Netflow文件用来保护用户隐私&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;作者在一台虚拟主机上设置了僵尸网络程序，这台主机唯一产生僵尸网络流量。然后作者将该虚拟主机和校园网络桥接，并分别在虚拟主机上抓包和校园网络某一台路由器上抓包。虚拟主机产生的流量是用于做类标记的。作者相信最好的办法是抓真实攻击的数据包，因此作者并没有在互联网入口处设置过滤。这样做真的好吗？&lt;/p&gt;

&lt;p&gt;作者利用Argus软件将Pcap文件转换为Netflow文件。Netflow文件包括了以下一些字段：开始时间，结束时间，持续时间，源IP，目的IP,源端口，目的端口，目的IP，状态，SToS，总包数，总包大小。&lt;/p&gt;

&lt;h4&gt;5.对比结果&lt;/h4&gt;

&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;BClus&lt;/strong&gt; showed large FPR values on most scenarios but also large TPR.
The &lt;strong&gt;CAMNEP&lt;/strong&gt; method had a low FPR during most of the scenarios but at the expense of a low TPR.
The &lt;strong&gt;BotHunter&lt;/strong&gt; algorithm presented very low values during the whole scenario despite that there were ten bots being executed.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt; 看来&lt;strong&gt;BotHunter&lt;/strong&gt;的检测效果在作者所做的实验中表现一般，因此作者在结论中评价也相对含蓄。&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;BotHunter method showed that in real environments it could still be useful to have blacklists of known malicious IP addresses&lt;/p&gt;&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;blockquote&gt;&lt;p&gt;Written with &lt;a href=&quot;https://stackedit.io/&quot;&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;&lt;/blockquote&gt;

&lt;h3&gt;论文[4]阅读笔记&lt;/h3&gt;

&lt;p&gt;一个区分现代Botnet和之前的僵尸网络的特征是结构化的覆盖拓扑结构使用的增加。结构化拓扑结构有利于增加botnet系统的稳定性，但同时也对其检测带来了新的突破口。&lt;/p&gt;

&lt;p&gt;论文作者提出了BotGrep检测算法，只通过peer节点之间的通信图来检测僵尸网络的节点。&lt;/p&gt;

&lt;p&gt;现有检测面临的挑战：随机端口，内容加密。背景流量越来越纷繁复杂，流量大数据导致算法需要重新设计。&lt;/p&gt;

&lt;p&gt; BotGrep算法输入：通信图，误用检测结果。输入误用检测结果的目标是为了区分P2P通信和Botnet。论文作者利用了一个“结构图”的特性叫做: &lt;strong&gt;&lt;em&gt;fast mixintg time：&lt;/em&gt;&lt;/strong&gt; i.e,the convergence time of random walks to a stationary distribution。来将僵尸网络的结构图从其余的通信图中分离出来。&lt;/p&gt;

&lt;p&gt;设原通信图为 \(G \subset =（V,E）\), \(G_p\)是在\(G\)中的P2P网络通信图 \(G_p \subset G \), \(G_n=G-G_p \)就是non-P2P通信图。作者最主要的idea就是从\(G \)中，分离出\(G_n\),\(G_p\)。基本思想就是因为P2P网络的拓扑结构是存在结构规律的，比背景的网络流量更具有结构性。在随机游走的情况下，P2P网络的mixing rate要比背景网络mixing rate更快。&lt;/p&gt;

&lt;p&gt;作者的主要步骤包括：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;过滤操作：开始输入的图可能包括百万级别的节点数量，在这一步中先抽取出小部分的P2P候选节点和false positive节点。&lt;/li&gt;
&lt;li&gt;运用&lt;strong&gt;&lt;em&gt;SybilInfer&lt;/em&gt;&lt;/strong&gt;聚类算法来对上一步中的P2P节点中的false positive节点去掉。&lt;/li&gt;
&lt;li&gt;最后一步用fast-mixing特征对P2P网络监测结果进行验证。&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;下面详细讲解一下每一步：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;过滤操作:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h4&gt;参考文献&lt;/h4&gt;

&lt;p&gt;[1]Dainotti A, Pescape A, Claffy K C. Issues and future directions in traffic classification[J]. Network, IEEE, 2012, 26(1): 35-40.&lt;/p&gt;

&lt;p&gt;[2]An empirical comparison of botnet detection methods&quot; Sebastian Garcia, Martin Grill, Honza Stiborek and Alejandro Zunino. Computers and Security Journal,
Elsevier. 2014. Vol 45, pp 100-123. http://dx.doi.org/10.1016/j.cose.2014.05.011&lt;/p&gt;

&lt;p&gt;[3]Ertoz L, Eilertson E, Lazarevic A, Tan PN, Kumar V, Srivastava J, et al.
Minds-minnesota intrusion detection system. In: Next generation data mining. MIT Press; 2004. pp. 199e218.&lt;/p&gt;

&lt;p&gt;[4]Nagaraja S, Mittal P, Hong C Y, et al. BotGrep: Finding P2P Bots with Structured Graph Analysis[C]//USENIX Security Symposium. 2010: 95-110.&lt;/p&gt;
</description>
        <pubDate>Fri, 08 May 2015 00:00:00 +0800</pubDate>
        <link>http://jasonzhuo.com///botnet-detection/</link>
        <guid isPermaLink="true">http://jasonzhuo.com///botnet-detection/</guid>
      </item>
    
      <item>
        <title>Shadowsocks in Lab</title>
        <description>&lt;p&gt;&lt;img src=&quot;https://dn-teddysun.qbox.me/wp-content/uploads/2015/shadowsocks_logo.png&quot; alt=&quot;image&quot; /&gt;
好多童鞋搞不清楚我在实验室搭建的Shadowsocks如何使用，在这里给你们科普一下，知道原理之后使用起来就更加顺心了，保证一口气科研就到天亮哦~&lt;/p&gt;

&lt;!-- more --&gt;


&lt;h4&gt;注明：以下高能内容转载自&lt;a href=&quot;http://vc2tea.com/whats-shadowsocks/&quot;&gt;vc2tea的博客&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;在很久很久以前，我们访问各种网站都是简单而直接的，用户的请求通过互联网发送到服务提供方，服务提供方直接将信息反馈给用户.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://vc2tea.com/public/upload/whats-shadowsocks-01.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;然后有一天，GFW 就出现了，他像一个收过路费的强盗一样夹在了在用户和服务之间，每当用户需要获取信息，都经过了 GFW，GFW将它不喜欢的内容统统过滤掉，于是客户当触发 GFW 的过滤规则的时候，就会收到 Connection Reset 这样的响应内容，而无法接收到正常的内容.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://vc2tea.com/public/upload/whats-shadowsocks-02.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;聪明的人们想到了利用境外服务器代理的方法来绕过 GFW 的过滤，其中包含了各种HTTP代理服务、Socks服务、VPN服务, &lt;strong&gt;Tor&lt;/strong&gt;, &lt;strong&gt;Freegate&lt;/strong&gt; … 其中以 ssh tunnel 的方法比较有代表性&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;首先用户和境外服务器基于 ssh 建立起一条加密的通道&lt;/li&gt;
&lt;li&gt;用户通过建立起的隧道进行代理，通过 ssh server 向真实的服务发起请求&lt;/li&gt;
&lt;li&gt;服务通过 ssh server，再通过创建好的隧道返回给用户&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;img src=&quot;http://vc2tea.com/public/upload/whats-shadowsocks-03.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;由于 ssh 本身就是基于 RSA 加密技术，所以 GFW 无法从数据传输的过程中的加密数据内容进行关键词分析，避免了被重置链接的问题，但由于创建隧道和数据传输的过程中，ssh 本身的特征是明显的，所以 GFW 一度通过分析连接的特征进行干扰，导致 ssh 存在被定向进行干扰的问题。&lt;/p&gt;

&lt;p&gt;这时候&lt;strong&gt;shadowsocks&lt;/strong&gt;横空出世。简单看来，shadowsocks是将原来 ssh 创建的 Socks5 协议拆开成 server 端和 client 端，所以下面这个原理图基本上和利用 ssh tunnel 大致类似。&lt;/p&gt;

&lt;h4&gt;shadowsocks工作原理&lt;/h4&gt;

&lt;p&gt;1、6) 客户端发出的请求基于 Socks5 协议跟 ss-local 端进行通讯，由于这个 ss-local 一般是本机或路由器或局域网的其他机器，不经过 GFW，所以解决了上面被 GFW 通过特征分析进行干扰的问题&lt;/p&gt;

&lt;p&gt;2、5) ss-local 和 ss-server 两端通过多种可选的加密方法进行通讯，经过 GFW 的时候是常规的TCP包，没有明显的特征码而且 GFW 也无法对通讯数据进行解密&lt;/p&gt;

&lt;p&gt;3、4) ss-server 将收到的加密数据进行解密，还原原来的请求，再发送到用户需要访问的服务，获取响应原路返回&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://vc2tea.com/public/upload/whats-shadowsocks-04.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;截止目前，笔者所使用最流畅、最稳定的就是shadowsocks，一口气翻墙不费劲，以后再也不用操心翻墙了。传送门：&lt;a href=&quot;https://github.com/shadowsocks/shadowsocks&quot;&gt;shadowsocks下载地址&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;在我们实验室中，翻墙代理设置可以为网关的IP地址192.168.1.1，也可以为本地127.0.0.1地址。唯一区别就是上图中SS Local代理程序运行的位置不同，（刘博士懂了撒~）。在实验室环境中，童鞋们可以自己设置利用本机的代理，也可以使用阿江搭建在网关上的SS Local代理，效果一致。没有在实验室的童鞋就只能靠自己本地代理啦。最后使用的时候需要设置为Socks5代理类型就可以正常使用了。
&lt;img src=&quot;/assets/smilies/37.gif&quot; id=&quot;similey&quot;&gt;
(css inline 表情，深夜特别鸣谢刘博士)
&lt;img src=&quot;/assets/smilies/28.gif&quot; id=&quot;similey&quot;&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 25 Apr 2015 00:00:00 +0800</pubDate>
        <link>http://jasonzhuo.com///shadowsocks-in-lab/</link>
        <guid isPermaLink="true">http://jasonzhuo.com///shadowsocks-in-lab/</guid>
      </item>
    
      <item>
        <title>Botnet and fast flux</title>
        <description>&lt;p&gt;Botnet and fast flux 特征介绍；Botminer论文初看,僵尸网络论文阅读笔记（last update:2015.8.10）&lt;/p&gt;

&lt;!-- more --&gt;


&lt;h2&gt;Botnet and fast flux&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3&gt;1.几个定义&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;NS（Name Server）&lt;/strong&gt;：记录是权威域名服务器记录，用来指定该域名由哪个DNS服务器来进行解析。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A record&lt;/strong&gt; A记录是名称解析的重要记录，它用于将特定的主机名映射到对应主机的IP地址上。你可以在DNS服务器中手动创建或通过DNS客户端动态更新来创建。&lt;/p&gt;

&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Round-Robin DNS (RRDNS)&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Round-robin的意思是循环的意思，顾名思义，这种DNS返回的A记录不只一个，返回的记录是一个列表，列表里面记录的先后顺序是循环出现的。&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Content Distribution Networks (CDN)&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;中文名叫内容分发网络，和RRDNS比较类似，不过返回的记录的TTL值相对RRDNS更低。CDN系统能够实时地根据网络流量和各节点的连接、负载状况以及到用户的距离和响应时间等综合信息将用户的请求重新导向离用户最近的服务节点上。其目的是使用户可就近取得所需内容，解决 Internet网络拥挤的状况，提高用户访问网站的响应速度。&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Fast-Flux Service Networks (FFSN)&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;FFSN更加创新地利用了RRDNS和CDN的上述特性，来降低恶意服务器被发现和关闭的概率。FFSN的特点下面会讨论。&lt;/dd&gt;
&lt;/dl&gt;

&lt;hr /&gt;

&lt;h3&gt;2.Fast flux特征&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;不重复的IP地址数量&lt;/strong&gt;：通常情况来说合法DNS查询不重复的IP地址为1~3个，而fast flux查询结果中会有5~6个，以确保至少有一个IP可以连接。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NS数量&lt;/strong&gt;：NS数量是指在单一次DNS 查询中所得到的NS （Name server）数量。客户端与DNS 主机进行查询时,可能透过快速变动网域技术掩护DNS 主机,因此NS Records 与NS 的A Records 可能有多笔记录,相较之下,合法的FQDN 其NS Records 与NS 的A Records比较少。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ASN数量与注册时间&lt;/strong&gt;：指对ASN进行查询时，主机使用的IP所属的ASN是否属于同一个单位。由于CDN主机使用的IP所属的ASN多属于同一个单位，而fast flux主机大多分散在世界各地，与CDN向比较之下，主机使用的IP所属的ASN属于不同单位。注册时间能够缩小选取范围。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Domain age&lt;/strong&gt;:指合法网站记录的TTL时间相对于恶意网站更长；恶意网站的FQDN与对应的IP记录不会长时间存留电脑，电脑必须时常进行DNS查询，以更新记录。RFC1912建议TTL最小为1~5天，这么长！而FFSN的TTL值一般小于600秒。但是一般不使用TTL值来判定FFSN，因为这样的误报率比较高，合法使用的CDN也会返回比较低的TTL值。因此TTL值一般用来区分FFSN/CDN和RRDNS.&lt;/li&gt;
&lt;/ol&gt;


&lt;h4&gt;2.1 关于Fast flux的第一篇论文[3]&lt;/h4&gt;

&lt;p&gt;Fastflux网络和普通网络对比：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/fastflux.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Single flux网络和Double flux网络对比（Double flux比Single flux更加复杂）：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;doulbe flux 的A记录和NS记录都在变，而single flux网络的A记录变化而NS记录不变&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/singledoubleflux.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;攻击者采用fast flux的动机主要有:（1）简单：仅仅需要1台mothership主机来serve master content还有DNS信息。（2）front-end 节点是对于攻击者来说是可以部分随时放弃的。（3）延长关键的backend core servers的生命周期。&lt;/p&gt;

&lt;p&gt;真实网络中的Fast flux网络搭建过程：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;攻击者用伪造或者偷窃的银行卡信息购买一个域名（www.example.com）该域名可以和某银行的域名很相似（原因你懂的）。&lt;/li&gt;
&lt;li&gt;攻击者控制一部分主机群作为转发主机（Redirectors）或者临时租用一个僵尸网络。&lt;/li&gt;
&lt;li&gt;攻击者将Name server (NS)记录公布，要么指向公开的DNS服务器，要么指向受控的转发主机节点。&lt;/li&gt;
&lt;li&gt;搭建完成&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;在真实网络中的Fast flux技术主要使用在基于HTTP协议的botnet[3]。&lt;/p&gt;

&lt;h4&gt;常见僵尸网络检测程序&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt; Name        &lt;/th&gt;
&lt;th style=&quot;text-align:center;&quot;&gt;TYPE                       &lt;/th&gt;
&lt;th style=&quot;text-align:center;&quot;&gt;Protocols &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BotSniffer     &lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt; centralized servers     &lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt;   IRC,HTTP &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; BotHunter      &lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt; structure independent  &lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt;Protocol independent &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; Botminer &lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt; structure independent  &lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt;    Protocol independent       &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;h3&gt;3.BotMiner 初看&lt;/h3&gt;

&lt;h5&gt;BotMiner的系统架构&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2015-04-19-botminer.png&quot; alt=&quot;Structure of Botminer&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;论文中的一些定义&lt;/h4&gt;

&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;A平面&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;主要检测恶意行为模型,文章用Snort检测某主机的行为,主机的扫描行为用的是Bothunter的SCADE，下载地址：&lt;a href=&quot;http://www.bothunter.net/&quot;&gt;Cyber-TA.BotHunter Free Internet Distribution Page, 2008. &lt;/a&gt;。文章采用两种异常检测模式，一种是高频率异常扫描次数，另外一种是带加权的失败连接次数。&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;C平面&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;网络通信关系模型和流特征模型&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;AC跨平面关联&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;发现AC平面之间的某种关联关系，僵尸网络评分s(h)&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;C平面的聚类&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;  定义 C-flow 为一段时间E内具有相同源IP，目的IP和目的端口的网络流&lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;&lt;strong&gt;C-flow的一些特征&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;FPH: the number of flows per hour.&lt;/li&gt;
&lt;li&gt;PPF: the number of packets per flow&lt;/li&gt;
&lt;li&gt;BPP: the average number of bytes per packets&lt;/li&gt;
&lt;li&gt;BPS: the average number of bytes per second&lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;4.论文[2]阅读笔记&lt;/h3&gt;

&lt;p&gt;该论文和之前僵尸网络研究的不同之处在于，之前的survey文章主要集中研究不同僵尸网络的技术细节，例如架构，通信协议，检测方法。这些研究只是僵尸网络某一方面的partial understanding，很难从全面的角度来了解问题。&lt;/p&gt;

&lt;p&gt;文章根据僵尸网络的生命周期对僵尸网络进行建模。僵尸网络的生命周期（线性）如下图所示：每个状态都可以额外引入隐藏操作（Complementary Hidding Mechanism):例如&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Multi-hopping（多跳链路）通常是不同地区的多重代理。&lt;/li&gt;
&lt;li&gt;Ciphering(加密)：SpamThru,Zeus采用的加密通道进行通信，Phatbot利用的是WASTE P2P私有加密协议。&lt;/li&gt;
&lt;li&gt;Binary obfuscation(二进制混淆)：防止逆向分析得出僵尸网络程序的行为特征和其他信息&lt;/li&gt;
&lt;li&gt;polymorphism(变形):通常实现了多种版本的源代码，但是基本功能保持不变。这样就可以绕过基于特征码检测的杀毒软件。（Phabot,Zeus采用了这种方法）&lt;/li&gt;
&lt;li&gt;IP spoofing(IP 地址伪造)：在DoS攻击中经常使用。&lt;/li&gt;
&lt;li&gt;Email-spoofing: 用错误的发件人地址发送邮件，在钓鱼攻击中经常使用。&lt;/li&gt;
&lt;li&gt;Fast-flux: 对于Fast-flux的拥有者，重要的是要有大量的代理，以及代理位置的异质性。&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;img src=&quot;/assets/images/botnetlifecycle.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Conception阶段主要去设计和实现一个特定目标的僵尸网络。MEECES为僵尸网络常见的构建动机（Money, Entertainment, Ego, Cause, Entrance to social groups, Status）&lt;/li&gt;
&lt;li&gt;Recruitment阶段主要是将僵尸网络程序运行在一个传统的主机上。&lt;/li&gt;
&lt;li&gt;Interaction阶段主要包括（1）bots和botmaster之间的通信过程 （2）秘密通信链路框架，来维护和统计bots。&lt;/li&gt;
&lt;li&gt;Marketing阶段主要用于传播僵尸网络程序&lt;/li&gt;
&lt;li&gt;攻击执行：攻击包括DDoS,Spam,click fraud（Search engine spam）, phishing，Data stealing等。&lt;/li&gt;
&lt;li&gt;攻击成功&lt;/li&gt;
&lt;/ol&gt;


&lt;h5&gt;常见僵尸网络程序及其时间&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/botnetyear.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;参考文献&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.usenix.org/legacy/event/sec08/tech/full_papers/gu/gu_html/&quot;&gt;[1]BotMiner: Clustering Analysis of Network Traffic for
Protocol- and Structure-Independent Botnet Detection&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2]Rodríguez-Gómez R A, Maciá-Fernández G, García-Teodoro P. Survey and taxonomy of botnet research through life-cycle[J]. ACM Computing Surveys (CSUR), 2013, 45(4): 45.&lt;/p&gt;

&lt;p&gt;[3]Enemy K Y. Fast-Flux Service Networks[J]. The Honey net Project and Research Alliance, 2007.&lt;/p&gt;
</description>
        <pubDate>Sat, 25 Apr 2015 00:00:00 +0800</pubDate>
        <link>http://jasonzhuo.com///botnet-and-fast-flux/</link>
        <guid isPermaLink="true">http://jasonzhuo.com///botnet-and-fast-flux/</guid>
      </item>
    
      <item>
        <title>Structure Risk Minimization,SRM</title>
        <description>&lt;p&gt;&lt;img src=&quot;/assets/images/2015-04-18-StatML.png&quot; alt=&quot;image&quot; /&gt;
本周学术交流，张老师给我们介绍了结构风险最小化原理，这篇博客对交流内容进行了精炼和总结。&lt;/p&gt;

&lt;!-- more --&gt;


&lt;h2&gt;结构风险最小化&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;基于数据的机器学习有2个方法，第1个是经典的统计估计方法，通过训练样本来估计参数值，代表是R.A Fisher 的统计理论（线性回归）。但我们经常做的预测真的&lt;strong&gt;靠谱&lt;/strong&gt;吗？答案是，不靠谱，因为我们在做预测的时候，引入了一个很强的假设条件———样本集要满足独立同分布条件（iid-independent identically distribulted）。&lt;/p&gt;

&lt;p&gt;第2个方法是V.Vapnik等人提出的统计学习理论。该理论也被称作VC理论，最重要的概念就是VC维。VC维~函数复杂度~是一个正整数（也叫做函数容量）。函数复杂度越大，预测效果越差。&lt;/p&gt;

&lt;p&gt;$$R_{exp}=\int L(y,f)p(x,y)dxdy \leq \frac{1}{m} \sum_i^{m} {(y_i-f(x_i))}^2 +\psi(\frac{V(f)}{m})$$&lt;/p&gt;

&lt;p&gt;其中\(V(f)\)就是函数VC维，m为样本个数。上式表明在样本一定的情况下，VC维越高，期望风险越大。&lt;/p&gt;

&lt;p&gt;统计学习理论-结构风险最小化SRM:&lt;br/&gt;
        $$期望风险&amp;lt;=经验风险 + 置信度$$&lt;/p&gt;

&lt;p&gt;这里简单介绍一下Vladimir Vapnik。Vladimir Vapnik是俄国人，1936年出生。1964年，他于莫斯科的控制科学学院获得博士学位。毕业后，他一直在该校工作直到1990年，这期间他作为不多（估计在国内可能有点那个），但是1991加入贝尔实验室之后，他在1995年发明了SVM。从此，他就出名了，2006年当上了美国科学院院士。
&lt;img src=&quot;/assets/smilies/16.gif&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;所谓的结构风险最小化就是在保证分类精度（经验风险）的同时，降低学习机器的 VC 维，可以使学习机器在整个样本集上的期望风险得到控制。&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;见图：
&lt;img src=&quot;http://img.my.csdn.net/uploads/201105/29/0_1306659245j5ZS.gif&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;张老师随后介绍了SVM中最著名的核方法。将低维度转化到高维度，计算复杂度却和低维度差不多。这太牛了，佩服Mercer。张老师补充到，这个是1911年提出来的（当时清王朝刚被推翻，国内可能无人潜心科研吧），之后Mercer的论文成为了睡美人，直到后来被Vapnik发现。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;核函数&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$$&amp;lt;\psi(x),\psi(y)&gt;=k(x,y)$$&lt;/p&gt;

&lt;p&gt;核函数的作用&lt;/p&gt;

&lt;p&gt;$$R^{n} \to H^{\infty}$$
(H为Hilbert空间)&lt;/p&gt;

&lt;p&gt;非线性问题&lt;strong&gt;一定&lt;/strong&gt;能够线性化&lt;/p&gt;
</description>
        <pubDate>Sat, 18 Apr 2015 00:00:00 +0800</pubDate>
        <link>http://jasonzhuo.com///structure-risk-minimizationsrm/</link>
        <guid isPermaLink="true">http://jasonzhuo.com///structure-risk-minimizationsrm/</guid>
      </item>
    
      <item>
        <title>Big data Short notes</title>
        <description>&lt;p&gt;Bigdata培训课程，听了一天，感觉听不大懂，工程细节上的东西太多了，而且自己这方面也刚刚起步，因此本文就稍微记一下我比较感兴趣的内容。&lt;/p&gt;

&lt;!-- more --&gt;


&lt;h3&gt;Spark Streaming&lt;/h3&gt;

&lt;blockquote&gt;&lt;p&gt;目前的大数据处理可以分为如下3个类型：&lt;/p&gt;&lt;/blockquote&gt;

&lt;ol&gt;
&lt;li&gt;复杂的批量数据处理：10min~数小时&lt;/li&gt;
&lt;li&gt;基于历史数据的交互式查询： 10sec~ 数分钟&lt;/li&gt;
&lt;li&gt;基于实时数据流的数据处理（Streaming data processing): 数百毫秒到数秒&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;除了Spark，流式计算计算系统比较有名的包括Twitter Storm和Yahoo S4。现在所提及的Storm主要是指Apache Storm ，Apache Storm的前身是 Twitter Storm 平台，目前已经归于 Apache 基金会管辖。Storm已经出现好多年了，而且自从2011年开始就在Twitter内部生产环境中使用，还有其他一些公司。而Spark Streaming是一个新的项目, 2013年开始。&lt;/p&gt;

&lt;p&gt;Spark的流式计算还是要弱于Storm的，作者在&lt;a href=&quot;http://www.csdn.net/article/2014-08-04/2821018&quot;&gt;这篇文章中&lt;/a&gt;认为互联网公司对于Storm的部署还是多于Spark。这篇文章对&lt;a href=&quot;http://blog.csdn.net/anzhsoft/article/details/38168025&quot;&gt;流式计算&lt;/a&gt;系统的设计考虑的一些要素进行了比较详细的讨论。这篇文章介绍了Storm和Streaming框架的&lt;a href=&quot;http://www.open-open.com/lib/view/open1426129553435.html&quot;&gt;对比&lt;/a&gt;. 如此说来，Storm在以后的项目中估计要用到&lt;img src=&quot;/assets/smilies/8.gif&quot; alt=&quot;image&quot; /&gt;.&lt;/p&gt;

&lt;p&gt;相对与&lt;strong&gt;Mapreduce&lt;/strong&gt;来说，Mapreduce的输入数据集合是静态的，不能动态变化。因此适合于离线处理。Mapreduce的使用场景包括，简单的网站pv,uv统计，搜索引擎建立索引，海量数据查找，复杂数据的分析和算法实现（聚类，分类，推荐，图算法等）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Yarn&lt;/strong&gt; 的提出，解决了多计算框架直接的数据无法共享问题，同时负责集群资源的统一管理和调度。&lt;/p&gt;

&lt;p&gt;运行在YARN上的计算框架：
1. 离线计算框架Mapreduce
2. DAG计算框架Tez
3. 流式计算框架Strom
4. 内存计算框架Spark&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Written with &lt;a href=&quot;https://stackedit.io/&quot;&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;&lt;/blockquote&gt;
</description>
        <pubDate>Sat, 18 Apr 2015 00:00:00 +0800</pubDate>
        <link>http://jasonzhuo.com///big-data-course/</link>
        <guid isPermaLink="true">http://jasonzhuo.com///big-data-course/</guid>
      </item>
    
  </channel>
</rss>